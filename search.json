[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workbench Developer’s Guide",
    "section": "",
    "text": "Preface\nThe Carpentries Workbench is a open-source and portable lesson infrastructure built with the R programming language. Despite it being built in R, contributors do not need to know any R in order to use it to build reliable, stylish, and accessible lessons.\nThis book serves as development documentation for The Carpentries Workbench. It was written between June and December 2023 primarily to orient new developers and contributors to The Workbench ecosystem. It was written in conjunction with ongoing development, training, and maintenance of The Carpentries Workbench and thus should be considered in a draft state.\n\n\n\n\n\n\nPrerequisite\n\n\n\nThis book assumes familiarity with R Package Development. If you are unfamiliar, please read R Packages (2e) (Wickham and Bryan 2023).\n\n\n\n\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. “R Packages (2e).” https://r-pkgs.org/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Building Lessons\nIn a broad sense, this is what happens when you run sandpaper::serve() or sandpaper::build_lesson(). The interaction between the three Workbench packages, the lesson content, and the author can be summarised like this where the author makes an edit:\nsequenceDiagram\n    autonumber\n    actor Author\n    participant Lesson \n    box rgb(255, 214, 216) The Workbench\n    participant {sandpaper}\n    participant {pegboard}\n    participant {varnish}\n    end\n\n    Author -&gt;&gt; {sandpaper}: sandpaper::serve()\n    activate Author\n    {sandpaper} --) Author: website preview\n    note left of {sandpaper}: monitor for changes\n    Author -&gt;&gt; Lesson: make an edit\n    deactivate Author\n    Lesson --&gt;&gt; {sandpaper}: READ changed file(s)\n    {sandpaper} --&gt;&gt; {pegboard}: validate Lesson\n    activate {pegboard}\n    note left of {sandpaper}: provision global menu elements\n    {pegboard} --) Author: report accessibility \n    deactivate {pegboard}\n    activate {sandpaper}\n    note left of {sandpaper}: WRITE markdown\n    {varnish} --&gt;&gt; {sandpaper}: load and apply website template\n    note left of {sandpaper}: WRITE website\n    {sandpaper} --) Author: website preview\n    deactivate {sandpaper}\nIn terms of folder structure, the workflow runs the two-step workflow to first render markdown files into site/built and then uses those files to render the HTML, CSS, and JavaScript into site/built. These workflows are detailed in The Workflows Chapter.\nflowchart TB\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2, color:#201434,stroke-width:1px\n\n    SERVE(\"serve()\"):::normal\n    BLESS(\"build_lesson()\"):::normal\n\n    subgraph \"Core Workflow\"\n    BUILT[\"site/built\"]:::local\n    SITE[\"site/docs\"]:::local\n    VLESS(\"validate_lesson()\"):::normal\n    BUILDMD([\"build_markdown()\"]):::normal\n    BUILDSITE([\"build_site()\"]):::normal\n    end\n\n    %%BUILT ~~~ SITE\n\n    SERVE --&gt; BLESS\n    %% SERVE ~~~ VLESS\n    %% SERVE ~~~ BUILDMD\n    BLESS --&gt; VLESS\n    VLESS -.- BUILDMD\n    BLESS --&gt; BUILDMD\n    BUILDMD --&gt; BUILT\n    BUILT -.- BUILDSITE\n    VLESS -.- BUILDSITE\n    BLESS --&gt; BUILDSITE\n    BUILDSITE --&gt; SITE\nThe site/docs folder contains the full website that can be safely used offline. This is the core of the workflow and is used both locally and in a remote setting. The only difference with the remote setting is that we use a few Git tricks to provision the markdown cache without needing to store it in the default branch.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-local",
    "href": "intro.html#sec-local",
    "title": "1  Introduction",
    "section": "",
    "text": "Summary Content\n\n\n\nThis content is a general picture of what happens between the packages. For a more in-depth discussion and more detailed diagrams, please visit the Flow Diagrams page.\n\n\n\n\n\n\n\n\n\n\n\nResource folder names\n\n\n\nThe names of the folders inside site/ are considered internal resources and they can change at any time. The reason why the folder for the final website output is called site/docs/ is because we use the {pkgdown} package to provision the website without needing to bundle the templates inside of {sandpaper}, but we never got around to explicitly changing the name of that folder.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-remote",
    "href": "intro.html#sec-remote",
    "title": "1  Introduction",
    "section": "1.2 Building Lessons Remotely (e.g. on GitHub)",
    "text": "1.2 Building Lessons Remotely (e.g. on GitHub)\nIn the remote workflow, we still use the same workflow as above, except now we use ci_deploy() to link the branches and folders using worktrees, which you can think of as Git branches assigned to separate folders.\n\n\n\n\n\n\nPlatform Independence\n\n\n\nWhen we developed The Workbench, GitHub was the most widely used platform for social coding that represented the easiest way for newcomers to contribute to our lessons. We used this knowledge to build the workflows for the lessons on GitHub, but we were also aware of the valid criticisms of GitHub and the dangers of vendor lock-in.\nThus, while the lessons are deployed using GitHub workflows and we have features that handle pull requests and updates, the core deployment features remain platform-independent. The workflows are merely instructions that we provide for GitHub to set up the workbench and to run the individual functions. In theory, any platform can be configured to deploy lessons via The Workbench.\nIn fact, in a pinch when GitHub workflows are not working properly, a lesson maintainer could run sandpaper:::ci_deploy() to render and deploy a local copy of the lesson.\n\n\n\n\n\n\n\nflowchart TB\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n\n    GH[(\"@main\")]:::remote\n    MDOUT[(\"@md-outputs\")]:::notouch\n    PAGES[(\"@gh-pages\")]:::notouch\n    DEPLOY([\"ci_deploy()\"]):::external\n    CIBUILDMD([\"ci_build_markdown()\"]):::external\n    CIBUILDSITE([\"ci_build_site()\"]):::external\n\n    subgraph virtual machine\n    REPO[\"[repo]\"]:::local\n    BUILT[\"[repo]/site/built\"]:::local\n    SITE[\"[repo]/site/docs\"]:::local\n    VLESS(\"validate_lesson()\"):::normal\n    BUILDMD([\"build_markdown()\"]):::normal\n    BUILDSITE([\"build_site()\"]):::normal\n    end\n\n\n    GH ---&gt; REPO\n    GH ~~~ DEPLOY\n    REPO -.- VLESS\n\n\n    DEPLOY ---&gt; VLESS\n    DEPLOY ---&gt; CIBUILDMD\n    DEPLOY ---&gt; CIBUILDSITE\n    VLESS -.- BUILDMD\n    CIBUILDMD ---&gt; MDOUT\n    MDOUT &lt;-.-&gt; BUILT\n    CIBUILDMD ---&gt; BUILDMD\n    CIBUILDSITE ---&gt; PAGES\n    PAGES &lt;-.-&gt; SITE\n    CIBUILDSITE ---&gt; BUILDSITE\n    BUILT -.- BUILDSITE\n    VLESS -.- BUILDSITE\n    BUILDMD --&gt; BUILT\n    BUILDSITE --&gt; SITE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#development",
    "href": "intro.html#development",
    "title": "1  Introduction",
    "section": "1.3 Development",
    "text": "1.3 Development\nDevelopment of The Workbench is overseen by Zhian N. Kamvar. New features are added incrementally as pull requests. Pushes to the main branch are rare and discouraged. New features must have tests associated (with the exception of {varnish}).\nIf you are interested, we have documentation for the release process available.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#documentation",
    "href": "intro.html#documentation",
    "title": "1  Introduction",
    "section": "1.4 Documentation",
    "text": "1.4 Documentation\nReference documentation for individual functions for each package is written alongside the function using {roxygen2}.\nThis documentation is generated by devtools::document()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#testing",
    "href": "intro.html#testing",
    "title": "1  Introduction",
    "section": "1.5 Testing",
    "text": "1.5 Testing\nTests for each package live in tests/testthat/ and follow a test-[file-name].R naming convention. These are controlled by the {testthat} package and run by devtools::test().\nYou can find more information about testing the core packages in Testing The Workbench",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#continous-integration",
    "href": "intro.html#continous-integration",
    "title": "1  Introduction",
    "section": "1.6 Continous Integration",
    "text": "1.6 Continous Integration\nThe continous integration for each package tests on Ubuntu, MacOS, and Windows systems with the last five versions of R (same as the RStudio convention).\nMore information about the Continous Integration can be found in the Continuous Integration section of the testing section.\n\nComing up:\n\nTesting Pull Requests (Locally and on your fork)\nResources for R package development\nAdding functionality to {sandpaper}\nAdding functionality to {pegboard}\nAdding styling elements to {varnish}\nAdding functionality to carpentries/actions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "2  System Setup",
    "section": "",
    "text": "2.1 Software Tools\nDevelopment of Workbench components requires the same toolchain for working on lessons:\nIt is recommended to have the latest versions of R and pandoc available. You need at least git 2.28 for security purposes.\nR version\n---\n\n\nR version 4.3.3 (2024-02-29) -- \"Angel Food Cake\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under the terms of the\nGNU General Public License versions 2 or 3.\nFor more information about these matters see\nhttps://www.gnu.org/licenses/.\n\n\n\npandoc version\n---\n\n\npandoc 3.1.11\nFeatures: +server +lua\nScripting engine: Lua 5.4\nUser data directory: /home/runner/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web: https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\n\n\n\ngit version\n---\n\n\ngit version 2.43.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>System Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#software-tools",
    "href": "setup.html#software-tools",
    "title": "2  System Setup",
    "section": "",
    "text": "R\npandoc\nGit",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>System Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#r-packages",
    "href": "setup.html#r-packages",
    "title": "2  System Setup",
    "section": "2.2 R Packages",
    "text": "2.2 R Packages\nOnce you have these installed, make sure to install ALL of the dependencies for the workbench:\ninstall.packages(c(\"sandpaper\", \"pegboard\", \"varnish\", \"tinkr\"),\n  dependencies = TRUE,\n  repos = c(getOption(\"repos\"), \"https://carpentries.r-universe.dev\"))\n\n\n\n\n\n\nWorking on Linux?\n\n\n\nIf you are on Linux, you will run into a couple of fun aspects that you may already be familiar with, especially if you have ever tried to install bioinformatic software:\n\nhaving to also install some extra C libraries (which are akin to R packages, but for C), such as the xslt library.\nhaving to build all packages from source\n\nYou can find detailed instructions in The Sandpaper Setup Guide, but the relevant commands are below.\n\nSystem Dependencies\nHere is the gist for Ubuntu Users to get system dependencies set up. Use The Carpentries R-Universe API to get all of the system dependencies. Here’s how to do that via CURL:\ncurl https://carpentries.r-universe.dev/stats/sysdeps 2&gt; /dev/null | jq -r '.headers[0] | select(. != null)'\nThis list can be sent to apt-get install to install everything:\nsudo apt-get install -y \\\n  $(curl https://carpentries.r-universe.dev/stats/sysdeps 2&gt; /dev/null | jq -r '.headers[0] | select(. != null)') 2&gt; /dev/null \\\n  || echo \"Not on Ubuntu\"\n\n\nBinary Packages\nTo get binary packages for your system, I will admit that it’s slightly confusing because they bury the instructions for registering your system to use binaries in the admin pages and even then, it’s kinda long. The gist is that you need to do two things:\n\nset your HTTPUserAgent header to state your R version and platform\nadd the packagemanager CRAN-like repository to R’s options:\n\nHere’s a script that you can copy and paste into ~/.Rprofile which will be run every time you start R\nlocal({\n  # Set the default HTTP user agent to get pre-built binary packages\n  RV &lt;- getRversion()\n  OS &lt;- paste(RV, R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])\n  codename &lt;- sub(\"Codename.\\t\", \"\", system2(\"lsb_release\", \"-c\", stdout = TRUE))\n  options(HTTPUserAgent = sprintf(\"R/%s R (%s)\", RV, OS))\n\n  # register the repositories for The Carpentries and CRAN\n  options(repos = c(\n    carpentries = \"https://carpentries.r-universe.dev/\",\n    CRAN = paste0(\"https://packagemanager.posit.co/all/__linux__/\", codename, \"/latest\")\n  ))\n})\nWhen you have this set up, you can then install the workbench packages:\n# Install The Workbench and dependencies\ninstall.packages(c(\"sandpaper\", \"varnish\", \"pegboard\", \"tinkr\"), dep = TRUE)\n\n\n\nThe {sandpaper} package comes with the {usethis} package embedded (though this may change in the future). In addition, you will need the {devtools} for development.\nI would also highly recommend the {pandoc} package for managing pandoc versions (NOTE: this requires you to have a personal access token set up).\ninstall.packages(\"devtools\")\ninstall.packages(\"pandoc\")\nOnce you have devtools, be sure to run devtools::dev_sitrep() and usethis::git_sitrep() to make sure you have the tools to build The Workbench:\ndevtools::dev_sitrep()\n#&gt; ── R ───────────────────────────────────────────────────────────────────────────\n#&gt; • version: 4.3.0\n#&gt; • path: '/usr/lib/R/'\n#&gt; ── devtools ────────────────────────────────────────────────────────────────────\n#&gt; • version: 2.4.5\n#&gt; ── dev package ─────────────────────────────────────────────────────────────────\n#&gt; • package: &lt;unset&gt;\n#&gt; • path: &lt;unset&gt;\n#&gt; ✔ All checks passed\n\nusethis::git_sitrep()\n#&gt; Git config (global)\n#&gt; • Name: 'Zhian N. Kamvar'\n#&gt; • Email: 'zkamvar@gmail.com'\n#&gt; • Global (user-level) gitignore file: &lt;unset&gt;\n#&gt; • Vaccinated: FALSE\n#&gt; ℹ See `?git_vaccinate` to learn more\n#&gt; ℹ Defaulting to 'https' Git protocol\n#&gt; • Default Git protocol: 'https'\n#&gt; • Default initial branch name: 'main'\n#&gt; GitHub\n#&gt; • Default GitHub host: 'https://github.com'\n#&gt; • Personal access token for 'https://github.com': '&lt;discovered&gt;'\n#&gt; • GitHub user: 'zkamvar'\n#&gt; • Token scopes: 'gist, repo, user, workflow'\n#&gt; • Email(s): 'zkamvar@gmail.com (primary)', ...\n#&gt; Git repo for current project\n#&gt; ℹ No active usethis project\nCreated on 2023-05-30 with reprex v2.0.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>System Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#development-workflow",
    "href": "setup.html#development-workflow",
    "title": "2  System Setup",
    "section": "2.3 Development Workflow",
    "text": "2.3 Development Workflow\nThis development workflow is known as Test Driven Development in which a test is written before things work. This way, we can confirm that a bug is fixed once it passes the tests and we have confidence that it will not fail again.\n\nopen RStudio and switch to the project for the package you are working on\ncheckout a new branch for your feature/bug\nload package via devtools::load_all() or ctrl+shift+L ( use cmd on macOS) to load the package NAMESPACE\n(if needed) document (either via devtools::document() or ctrl+shift+D)\nrun tests (either via devtools::test() or ctrl+shift+T to run the entire test suite OR to test a single file, use the “run tests” button in a test file or run testthat::test_local(filter = '[FILE SLUG]')\nmodify tests for new functionality/bug fix\nadd functionality/bug fix and move to 3 unless you are ready to push\nrun check with devtools::check() or ctrl+shift+E",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>System Setup</span>"
    ]
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "3  Testing The Workbench",
    "section": "",
    "text": "3.1 Introduction\nEvery single package that runs code in the lesson infrastructure is tested before it ever reaches any lesson. This is important because we want to give the lesson authors and maintainers as much freedom as they need to write a lesson while maintaining predictability and integrity. We also want to give our community confidence that this system works.\nWhenever a new feature or bug fix is added to The Workbench, it is imperative that a test is associated and verified before it gets sent into production.\nTests can be run locally and via continuous integration. This page introduces some of the testing strategies used in The Workbench and the caveats that come with these strategies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing The Workbench</span>"
    ]
  },
  {
    "objectID": "testing.html#introduction",
    "href": "testing.html#introduction",
    "title": "3  Testing The Workbench",
    "section": "",
    "text": "The first stage of your testing journey is to become convinced that testing has enough benefits to justify the work. For some of us, this is easy to accept. Others must learn the hard way.\n— Wickham and Bryan, Testing Basics, R Packages second edition\n\n\nIf you use software that lacks automated tests, you are the tests.\n— Jenny Bryan source tweet (2018-09-22 01:13 UTC)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing The Workbench</span>"
    ]
  },
  {
    "objectID": "testing.html#sec-unit-tests",
    "href": "testing.html#sec-unit-tests",
    "title": "3  Testing The Workbench",
    "section": "3.2 Unit Testing",
    "text": "3.2 Unit Testing\nThe tests under test/testthat/ are run in alphabetical order using the {testthat} package (see https://r-pkgs.org/testing-basics.html) via devtools::test() or devtools::check().\n\n3.2.1 Conditionally Skipped Tests\nThe tests often need special conditions in order to run and sometimes those conditions are not possible. One of the most common conditions to skip is if the testing happens on CRAN. They are very hawkish about how long test suites can run and it’s often difficult to detect the state of a CRAN machine, so it’s better to skip long-running tests or those with complex environmental dependencies on CRAN (which does not yet apply to {sandpaper}).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing The Workbench</span>"
    ]
  },
  {
    "objectID": "testing.html#sec-ci",
    "href": "testing.html#sec-ci",
    "title": "3  Testing The Workbench",
    "section": "3.3 Continous Integration",
    "text": "3.3 Continous Integration\nAll the unit tests are run in continuous integration for every push and pull request that occurs. They also run every week. This provisions the current releases of the R package dependencies along with development versions of critical dependencies such as {renv}.\nIn continous integration, we run on with the following conditions to make sure it works not only on GitHub, but also on local user machines:\n\ntest coverage (no package structure) with released versions on Ubuntu Linux (though reporting is stalled)\nFor each platform (Ubuntu Linux, macOS, and Windows)\n\nR CMD check, which checks the structure of the package and documentation\nall run on these versions of R: current, devel, and two previous R versions\n\n\nBecause of occasional provisioning failures on macOS and Windows, we require only that Ubuntu Linux latest version passes check for merging pull requests.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing The Workbench</span>"
    ]
  },
  {
    "objectID": "testing.html#sec-integration",
    "href": "testing.html#sec-integration",
    "title": "3  Testing The Workbench",
    "section": "3.4 Lesson Integration Testing",
    "text": "3.4 Lesson Integration Testing\nUnit tests are great for testing all functionality using known and stable inputs, it is important to test using known inputs that are in a constant state of flux: live lessons. This is where The Workbench Integration Test comes in. It will run a weekly test on a defined set of lessons using the current development versions of varnish and sandpaper. These tests are useful for three purposes:\n\nongoing integrity for the workbench lessons lessons that use different features of The Workbench such as R code execution\nreal-world effects of new sandpaper and varnish versions (including those in pull requests)\ninspecting changes in HTML and markdown output\n\nThese lessons that we use are\n\nInstructor Training\n\nLesson with the most content, contributors, activity, and used features. This particular lesson is a bit of a stress test for the infrastructure.\n\nR for SocialScientists\n\nThis is one of the first R-based lessons to be transitioned and it uses the tidyverse as a dependency.\n\nWorkbench Documentation\n\nThe workbench documentation. If this doesn’t work, nothing will.\n\nRaster and Vector Geospatial Data with R\n\nThis lesson uses R packages that rely on a geospatial software stack, which can be complex. Failures here likely mean that there are problems with provisioning external C libraries.\n\nBioConductor RNAseq\n\nLesson using BioConductor R packages by people at BioConductor. If this does not work, then there likely is a provisioning problem between BioConductor and {renv}.\n\n\n\n3.4.1 Testing sandpaper and varnish pull requests\nTo test a pull request version, you can head over the the main workflow and use the button that says “Run Workflow”. When you want to test a varnish or sandpaper pull request, you can use the [REPO]#[PR] syntax (e.g. carpentries/sandpaper#429 to run sandpaper pull request 429) in the entry fields for varnish and sandpaper version. If you don’t have a pull request to work from, you can use the [REPO]@[REF] syntax (e.g. carpentries/sandpaper@test-this-thing to run sandpaper test-this-thing branch).\n\n\n3.4.2 Inspecting changes\nThe output for all the tests are stored in branches that are named respective for their test repositories. For example, datacarpentry/r-socialsci/markdown and datacarpentry/r-socialsci/site contain the markdown and HTML outputs for the R for Social Scientists lesson. By inspecting the diffs from the commits, you can see how the output has changed from run to run, which is useful if you are confirming that a feature will be automatically deployed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing The Workbench</span>"
    ]
  },
  {
    "objectID": "testing.html#sec-ad-hoc",
    "href": "testing.html#sec-ad-hoc",
    "title": "3  Testing The Workbench",
    "section": "3.5 Ad-hoc Testing",
    "text": "3.5 Ad-hoc Testing\nThere are times when you cannot automate your way through testing and you just have to suck it up and get your (virtual) hands dirty. You find yourself in this kind of situation if you are testing out GitHub workflows, GitHub actions, or if you are implementing a new feature and you need to see that it works reliably and safely. It’s this point in time when you use ad-hoc testing on a brand new lesson repository that you give yourself permission to mess around in and will delete when finished.\n\n3.5.1 GitHub Workflows\nA GitHub Workflow is a YAML document that lives inside the .github/workflows folder of a repository. This sets up the environment needed to build a lesson. When you debug these, ask yourself if you really want to update the GitHub Actions instead. Because these are copied to each lesson, they need to be updated in each lesson (which is accomplished through the automated pull request workflow). If you’ve determined that the workflow needs to be modified, you can modify them inside your test lesson until you get the desired results. Once that is done, copy them over to sandpaper/inst/workflows and add a NEWS item that’s under the heading ## CONTINUOUS INTEGRATION describing your change.\n\n\n3.5.2 GitHub Actions\nA GitHub Action is a single step in a GitHub Workflow and can be written in nearly any language.\nThe GitHub actions that are in carpentries/actions are written in BASH, node JavaScript, and R and cobbled together with YAML. When developing a new feature, work on a branch and then, in your shiny new test lesson, replace the @main in the GitHub Workflows to your branch name. This way, you can know immediately if the fix or feature worked without having to interrupt someone’s flow.\n\n\n3.5.3 New config items or settings\nIf you implement a new config item (e.g. a lang: tag), a temporary lesson is a great way to test it. To do so, you can use the sandpaper: or varnish: keys in your lesson config to specify the version of sandpaper or varnish you want to test.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing The Workbench</span>"
    ]
  },
  {
    "objectID": "flow.html",
    "href": "flow.html",
    "title": "4  Flow Diagrams",
    "section": "",
    "text": "4.1 Introduction\nThis section builds on The broad workflow and details the internal process that are invoked with the sandpaper::build_lesson() function. If you look at the source for this function, it contains a total of sevens significant lines of code (many more due to documentation and comments).\nThe pre-flight steps all happen before a single source file is built. These check for pandoc, validate the lesson, and configure global elements. The last two lines are responsible for building the site and combining them with the global variables and templates.\nUsers will invoke this function in the following ways:\nAll of these methods will call sandpaper::validate_lesson() (which also sets up global metadata and menu variables) and the two-step internal functions sandpaper:::build_markdown() and sandpaper:::build_site(). Below, I break down and detail the process for each.",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flow Diagrams</span>"
    ]
  },
  {
    "objectID": "flow.html#introduction",
    "href": "flow.html#introduction",
    "title": "4  Flow Diagrams",
    "section": "",
    "text": "venue\nfunction\npurpose\n\n\n\n\nlocal\nsandpaper::build_lesson()\nrender content for offline use\n\n\nlocal\nsandpaper::serve()\ndynamically render and preview content\n\n\nremote\nsandpaper:::ci_deploy()\nrender content and deploy to branches",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flow Diagrams</span>"
    ]
  },
  {
    "objectID": "flow.html#preflight-checks",
    "href": "flow.html#preflight-checks",
    "title": "4  Flow Diagrams",
    "section": "4.2 Preflight Checks",
    "text": "4.2 Preflight Checks\nBefore a lesson can be built, we need to confirm the following:\n\nWe have access to the tools needed to build a lesson (e.g. pandoc). This is achieved via the sandpaper::check_pandoc()\nWe are inside a lesson that can be built with The Carpentries Workbench",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flow Diagrams</span>"
    ]
  },
  {
    "objectID": "flow.html#validate-lesson",
    "href": "flow.html#validate-lesson",
    "title": "4  Flow Diagrams",
    "section": "4.3 validate_lesson()",
    "text": "4.3 validate_lesson()\nThe lesson validator is a bit of a misnomer. Yes, it does peform lesson validation, which it does so through the methods in the pegboard::Lesson R6 class.\nIn order to use thse methods, it first loads the lesson, via the sandpaper::this_lesson() function, which loads and caches the pegboard::Lesson object. It also caches elements that are mostly duplicated across episodes with small tweaks for each episode:\n\nmetadata in JSON-LD format\nsidebar\nextras menu for learner and instructor views\ntranslations of menu elements defined in {varnish}",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flow Diagrams</span>"
    ]
  },
  {
    "objectID": "flow.html#build-markdown",
    "href": "flow.html#build-markdown",
    "title": "4  Flow Diagrams",
    "section": "4.4 build_markdown()",
    "text": "4.4 build_markdown()\n\n4.4.1 Generating Markdown\nMarkdown generation for the lesson is controlled by the internal function sandpaper:::build_markdown().\nWhen a lesson contains R Markdown files, these need to have content rendered to markdownsot hat we can further process them. This content is processed with the {knitr} R package in a separate R process. Markdown source content on the other hand is copied to the site/built folder.\nBecause R Markdown files can take some time to render, we use MD5 sums of the episode contents (stored in the site/built/md5sum.txt file) to skip any files that have not changed.\n\n\n\n\n\nsequenceDiagram\n    autonumber\n    participant episodes/episode.Rmd\n    box rgb(255, 214, 216) The Workbench\n    participant {sandpaper}\n    end\n    box rgb(230, 234, 240) Document Engine\n    participant {renv}\n    participant {knitr}\n    end\n    box rgb(255, 231, 168) Generated Files\n    participant site/built/md5sum.txt\n    participant site/built/episode.md\n    end\n\n    site/built/md5sum.txt --&gt;&gt; {sandpaper}: READ file cache\n    {sandpaper} --&gt;&gt; {knitr}: RUN conversion\n    episodes/episode.Rmd --&gt;&gt; {knitr}: PROCESS changed file(s)\n    {knitr} --&gt;&gt; site/built/episode.md: WRITE Markdown\n    {sandpaper} --&gt;&gt; site/built/md5sum.txt: WRITE file cache\n\n\n\n\n\n\n\n\n\n\n\n\nPackage Cache and Reproducibility\n\n\n\nOne package that is missing from the above diagram is {renv} and that’s partially because it has an indirect effect on the lesson: it provisions the packages needed to build the lesson.\nWhen episodes are rendered from R Markdown to Markdown, we attempt to reproduce the build environment as closely as possible by using the {renv} package. If the global package cache from {renv} is available, then the lesson profile is activated before the episode is sent to {knitr} and R will use the packages provided in that profile. This has two distinct advantages:\n\nThe user does not have to worry about overwriting packages in their own library (i.e. a graduate researcher working on their dissertation does not want to have to rewrite their analyses because of a new version of {sf})\nThe package versions will be the same as the versions on the GitHub version of the site, which means that there will be no false positives of new errors popping up\n\nFor details on the package cache, see the Building Lessons With A Package Cache article.\n\n\nAt this step, the markdown has been written and the state of the cache is updated so if we re-run this function, then it will show that no changes have occured. After this step, the internal function sandpaper:::build_site() is run where the markdown file that we just created is converted to HTML with pandoc and stored in an R object. This R object is then manipulated and then written to an HTML file with the {varnish} website templates applied.\nWe use this function in the pull request workflows to demonstrate the changes in markdown source files, which is useful when package versions change, causing the output to potentially change.",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flow Diagrams</span>"
    ]
  },
  {
    "objectID": "flow.html#build-site",
    "href": "flow.html#build-site",
    "title": "4  Flow Diagrams",
    "section": "4.5 build_site()",
    "text": "4.5 build_site()\nThe following sections will discuss the HTML generation (the following section), manipulation (the section after that), and applying the template (the final section) separately because, while these processes are each run via the internal sandpaper:::build_site() function, they are functionally separate.\n\n4.5.1 Generating HTML\nEach markdown file is processed into HTML via pandoc and returned to R as text. This is done via the internal function sandpaper:::render_html().\n\n\n\n\n\nsequenceDiagram\n    autonumber\n    box rgb(255, 214, 216) The Workbench\n    participant {sandpaper}\n    end\n    box rgb(230, 234, 240) Document Engine\n    participant pandoc\n    end\n    box rgb(255, 231, 168) Generated Files\n    participant site/built/episode.md\n    end\n\n    {sandpaper} --&gt;&gt; pandoc: LOAD pandoc with lua filters\n    site/built/episode.md --&gt;&gt; pandoc: READ markdown\n    pandoc --&gt;&gt; {sandpaper}: RENDER HTML as text\n\n\n\n\n\n\nFrom here, the HTML exists as the internal body content of a website without a header, footer, or any styling. It is nearly ready for insertion into a website template. The next section details the flow we use to tweak the HTML content.\n\n\n4.5.2 Processing HTML\nThe HTML needs to be tweaked because the output from pandoc, even with our lua filters, still needs some modification. We tweak the content by first converting the HTML into an Abstract Syntax Tree (AST). This allows us to programmatically manipulate tags in the HTML without resorting to using regular expressions.\nIn this part, we update links, images, headings, structure that we could not fix using lua filters. We also apply translations to some of the menu elements that are not templated in {varnish}. We then use the information from the episode to complete the global menu variable with links to the second level headings in the episode.\n\n\n\n\n\nsequenceDiagram\n    autonumber\n    box rgb(255, 214, 216) The Workbench\n    participant {sandpaper}\n    end\n    box rgb(230, 241, 255) R Object\n    participant HTML(AST)\n    end\n    box rgb(230, 234, 240) Helper Package\n    participant {xml2}\n    end\n\n    {sandpaper} --&gt;&gt; {xml2}: READ HTML\n    {xml2} --&gt;&gt; HTML(AST): PARSE HTML\n    activate {sandpaper}\n    note right of HTML(AST): sandpaper:::fix_nodes()\n    {xml2} --&gt;&gt; HTML(AST): update structure\n    HTML(AST) --&gt;&gt; {sandpaper}: extract menu items\n    note right of {sandpaper}: generate learner and instructor versions\n    deactivate {sandpaper}\n\n\n\n\n\n\n\n\n\n\n\n\nWorking With XML\n\n\n\nWorking with XML data is perhaps one of the strangest experiences for an R user because in R, functions will normally return a copy of the data, but when working with an XML document parsed by {xml2}, the data is modified in place.\nIt allows us to do neat things, but there is a learning curve associated.\nI have written hopefully helpful handbooks (guides) on\n\nworking with XML data,\nworking with the Lesson object and\nworking with the Episode object\n\n\n\n\n\n4.5.3 Applying Website Template\nNow that we have an HTML AST that has been corrected and associated metadata, we are ready to write this to HTML. This process is achieved by passing the AST and metadata to {pkgdown} where it performs a little more manipulation, applies the {varnish} template, and writes it to disk.\n\n\n\n\n\nsequenceDiagram\n    autonumber\n    box rgb(255, 214, 216) The Workbench\n    participant {sandpaper}\n    participant {varnish}\n    end\n    box rgb(230, 241, 255) R Object\n    participant HTML(AST)\n    end\n    box rgb(230, 234, 240) Helper Package\n    participant {pkgdown}\n    end\n    box rgb(255, 231, 168) Generated Files\n    participant site/docs/episode.html\n    end\n\n    activate {sandpaper}\n    {sandpaper} --&gt;&gt; {pkgdown}: Set global menu variables\n    HTML(AST) --&gt;&gt; {pkgdown}: Hand off HTML to pkgdown\n    deactivate {sandpaper}\n    activate {pkgdown}\n    {varnish} --&gt;&gt; {pkgdown}: Load template\n    {pkgdown} --&gt;&gt; site/docs/episode.html: WRITE website\n    deactivate {pkgdown}",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flow Diagrams</span>"
    ]
  },
  {
    "objectID": "features.html",
    "href": "features.html",
    "title": "5  Adding Features",
    "section": "",
    "text": "5.1 Introduction\nOne of the best things about developing a tool for a broad community: there is never a dearth of ideas that will fuel the improvement of the tool. This fact also happens to be one of the worst things about developing a tool for a broad community. The larger and more widely-used a tool is, the more you begin to find that the default features really shape the tool, which shapes what people can create with it.\nThe workbench was built to be a feature-complete replacement for The styles repository in the sense that the following was true:\nDuring the design phase (2020–2021), we explicitly focussed on a set of features that would enhance the above features of lessons. The most salient features that were of immediate value to our community were:\nOne key about all of these features? As The Workbench was being built, these were all implemented separately. This may not seem like a big deal, but it is worth noting because it gives creedence to the modular nature of The Workbench. As I work on The Workbench I always strive to retain this modularity and ensure that whatever I create does not break the workflow of another (unless I have a very good reason for it).",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "features.html#introduction",
    "href": "features.html#introduction",
    "title": "5  Adding Features",
    "section": "",
    "text": "Lessons were comprised of primarily episodes with special pages that aggregate content including key points and images\nPages within lessons would have links back to their source content so people could edit the pages they were viewing\nLessons could be writtin in Markdown and deployed to GitHub pages\nLessons could be rendered and previewed locally\nLessons could render Markdown from R Markdown\n\n\n\nA web template that was tested for its accessibility\nA separate instructor and learner view\nAn internal package cache for R Markdown-based lessons",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "features.html#the-workbench-user-interface-is-a-lesson",
    "href": "features.html#the-workbench-user-interface-is-a-lesson",
    "title": "5  Adding Features",
    "section": "5.2 The Workbench user interface is a lesson",
    "text": "5.2 The Workbench user interface is a lesson\nWhen building a feature for The Workbench, it’s important to remember that people who use The Workbench may not know how to use R, Git, or GitHub. They will be working with a lesson and members of The Carpentries who have been here for a hot minute might actually be afraid of rendering a lesson locally from the experiences they had with the Jekyll interface.",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "features.html#ask-not-for-whom-the-feature-breaks",
    "href": "features.html#ask-not-for-whom-the-feature-breaks",
    "title": "5  Adding Features",
    "section": "5.3 Ask not for whom the feature breaks",
    "text": "5.3 Ask not for whom the feature breaks\nBefore adding a new feature to The Workbench, it is important to consider the answer to a few questions before even drafting the implementation, which are detailed later in this section. These questions boil down to thinking deeply about purpose, users, support, and resources, which are all interlinked. Broadly, features can be thought of in two categories: non-breaking changes and breaking changes. When given the choice, always strive for the non-breaking change.\n\n5.3.1 Non-breaking changes\nMost of the features you will encounter fall into this category. These are little gifts that you give to your users when they update The Workbench. Sometimes, they can be as small as an message that is formatted to be more readable. Often times, they are going to be optional flags in config.yaml that will allow the user to customize their lesson just a little bit.\n\n\n5.3.2 Breaking changes\nFor example, the transition to The Workbench represents a feature with breaking changes whose needs outweighed the negative impact for some of the users. This was a project whose purpose was to improve the accessibility and maintainability of our lessons for our community. It would impact the users by providing a better interface at the cost of requiring maintainers to fundamenally change their workflows. We were able to provide support for The Workbench because the developer of the system was a paid staff member of The Carpentries. Finally, the resources existed through funding via Chan-Zuckerberg Initiative, the Moore Foundation, The Sloan Foundation, and The R Consortium. Moreover the tools that we used to build The Workbench were well-tested and well-supported.",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "features.html#purpose",
    "href": "features.html#purpose",
    "title": "5  Adding Features",
    "section": "5.4 Purpose",
    "text": "5.4 Purpose\n\n5.4.1 What is the scope of the feature?\n\n\n5.4.2 What will this feature do?\n\n\n5.4.3 Why is this feature needed?\n\n\n5.4.4 Is the feature optional?",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "features.html#users",
    "href": "features.html#users",
    "title": "5  Adding Features",
    "section": "5.5 Users",
    "text": "5.5 Users\n\n5.5.1 Who will use this feature?\n\n\n5.5.2 Who will not use this feature?\n\n\n5.5.3 Who will be affected by this feature?",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "features.html#support",
    "href": "features.html#support",
    "title": "5  Adding Features",
    "section": "5.6 Support",
    "text": "5.6 Support\n\n5.6.1 Do we have the resources to implement most of this feature?\n\n\n5.6.2 Can we provide support in the future? AKA what is the bus factor?",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "features.html#resources",
    "href": "features.html#resources",
    "title": "5  Adding Features",
    "section": "5.7 Resources",
    "text": "5.7 Resources\n\n5.7.1 Is a feature dependent on another vendor?\n\n\n5.7.2 Can this feature be used in an area with limited internet connection?\n\n\n5.7.3 Is this feature going to significantly slow down build time?",
    "crumbs": [
      "Design Considerations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Adding Features</span>"
    ]
  },
  {
    "objectID": "sandpaper/intro.html",
    "href": "sandpaper/intro.html",
    "title": "6  The {sandpaper} package",
    "section": "",
    "text": "6.1 Introduction\nThe {sandpaper} package is the user interface for The Carpentries workbench and orchestrates the building and deployment of lessons. It helps lesson developers, authors, and maintainers to smooth out their contributions. Because of it’s user-facing and modular nature, it is the most complex package in The Workbench.\nPeople who want to use {sandpaper} will generally use it for one of these five things1:\nImportantly, all of these points must be achievable by someone with little to no experience with R or any other programming language.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The {sandpaper} package</span>"
    ]
  },
  {
    "objectID": "sandpaper/intro.html#introduction",
    "href": "sandpaper/intro.html#introduction",
    "title": "6  The {sandpaper} package",
    "section": "",
    "text": "sandpaper (n.)\nHeavy paper coated on one side with sand or other abrasive material and used for smoothing surfaces.\n\n\n\n\nCreating lessons\nContributing to lessons\nMaintaining lessons\nRendering a portable lesson site\nRendering a lesson site with continous integration (GitHub Actions)",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The {sandpaper} package</span>"
    ]
  },
  {
    "objectID": "sandpaper/intro.html#not-a-static-site-generator",
    "href": "sandpaper/intro.html#not-a-static-site-generator",
    "title": "6  The {sandpaper} package",
    "section": "6.2 Not a static site generator",
    "text": "6.2 Not a static site generator\nOne important distinction I would like to make is that {sandpaper} is not a static site generator. Yes, it may act like a static site generator because it creates static sites that are portable from markdown sources. However, it differs in that it is not intended to be as flexible as many other static site generators. This may seem like a negative point, but in the context of Carpentries Lessons, it is an asset.\nMany static site generators are extremely flexible at the cost of requiring the user to think deeply about the model of the website structure, deployment, and maintenance. For a single website, this is fine, but for distributed lessons like those in The Carpentries, it is much more difficult to use a static site generator because lesson maintainers should only have to focus on the content, not the mechanics or style. The {sandpaper} package builds lesson websites and nothing more. It is possible to use it for a narrative analysis, but at the end of the day, it will still be a lesson website.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The {sandpaper} package</span>"
    ]
  },
  {
    "objectID": "sandpaper/intro.html#you-had-one-several-jobs",
    "href": "sandpaper/intro.html#you-had-one-several-jobs",
    "title": "6  The {sandpaper} package",
    "section": "6.3 You had one several jobs",
    "text": "6.3 You had one several jobs\nIn terms of it’s relation to the other Workbench packages, {sandpaper} depends on {pegboard} to validate lessons, extract questions and timings for the schedule, and to extract the code handout. It relies on {varnish} to provide the HTML, CSS, and JS templates that create the websites via {pkgdown}. Its other dependencies are varied in purpose. It relies on other R packages and pandoc to do much of the work.\n\n6.3.1 Building Websites\nAt it’s core, {sandpaper} provides a workflow to process R Markdown to Markdown (if the lesson is purely markdown-based, then it Markdown is simply copied), generate HTML, and package that HTML content into a website framework with the following ideals:\n\nWhen processing R Markdown, the user’s environment should not be modified\nGenerated content should be auditable, but not part of the main git history\nProcesses for generating markdown, HTML, and the website should be modular such that if a better tool comes along, it can serve as a drop-in replacement\n\n\n\n\n\n\nflowchart TB\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2, color:#201434,stroke-width:1px\n\n  subgraph Build Markdown\n  CR[\"{callr}\"]:::external\n  KT[\"{knitr}\"]:::external\n  RV[\"{renv}\"]:::external\n  LIB[\\\"renv/library\"/]:::external\n  MD[/\"markdown\"\\]:::local\n  end\n  \n  subgraph Render HTML\n  RMD[\"{rmarkdown}\"]:::external\n  pandoc([\"pandoc\"]):::default\n  HTML[(\"html (content)\")]:::local\n  end\n\n  subgraph Build Site\n  PD[\"{pkgdown}\"]:::external\n  VS[\"{varnish}\"]:::normal\n  SITE[/\"html (site)\"\\]:::remote\n  end\n\n  RMD -.-&gt;|provisions| pandoc\n  PD -.-&gt;|uses| VS\n  VS --&gt;|templates| SITE\n  MD --&gt;|input for| pandoc\n  pandoc --&gt;|outputs| HTML\n  CR -.-&gt;|loads| KT\n  CR -.-&gt;|loads| RV\n  RV -.-&gt;|provisions| LIB\n  LIB -.-&gt; KT\n  KT --&gt;|builds| MD\n  PD --&gt;|builds| SITE\n  HTML --&gt;|input for| PD",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The {sandpaper} package</span>"
    ]
  },
  {
    "objectID": "sandpaper/intro.html#footnotes",
    "href": "sandpaper/intro.html#footnotes",
    "title": "6  The {sandpaper} package",
    "section": "",
    "text": "As has been the case ever since the README was first written in August 2020 before any code was created. Fun fact, much of the README remains in tact and accurate: README 2020-08-04↩︎",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The {sandpaper} package</span>"
    ]
  },
  {
    "objectID": "sandpaper/testing.html",
    "href": "sandpaper/testing.html",
    "title": "7  Testing {sandpaper}",
    "section": "",
    "text": "7.1 Introduction\n{sandpaper} is the largest package and the main user and deployment interface for The Workbench. The tests are all designed to work within a lesson context, which means a couple of things to be aware of.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/testing.html#introduction",
    "href": "sandpaper/testing.html#introduction",
    "title": "7  Testing {sandpaper}",
    "section": "",
    "text": "Some tests will take a long time to run. IO procedures are often one of the most time-consuming steps in computing and {sandpaper} does a lot of it.\nMost tests will be dependent on the previous tests in a file. Because the tests work on a folder on disk, we need to provision different scenarios of lesson state. It is far simpler to do this by side-effect rather than copying, setting up, and tearing down the state of the lesson for each and every test.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/testing.html#test-setup",
    "href": "sandpaper/testing.html#test-setup",
    "title": "7  Testing {sandpaper}",
    "section": "7.2 Test Setup",
    "text": "7.2 Test Setup\nThere is nothing that you as the contributor/developer need to do to set up for running these tests beyond what you have already done in your development setup. This section describes conceptually how {testthat} and {sandpaper} setup the testing environment after you run devtools::test().\nIn short, this is the process:\n\n{sandpaper} is loaded\nHelper test files are loaded\nThe setup script is loaded, which provisions the test lesson\nEach test file matchingtests/testthat/test-*.R is run, resetting the test lesson at the top of each file.\n\n\n7.2.1 Test Helpers\nSandpaper has a few test helpers that handle some of the more tedious side-effects of testing.\n\nhelper-child.R\n\nSets up an episode that contains a child document for testing.\n\nhelper-hash.R\n\nExpectation expect_hashed() that an episode file MD5 sum (expected) matches the MD5 sum (actual) we recorded in the site/built/md5sum.txt database.\n\nhelper-processing.R\n\nProvides an output string that demonstrates a screen output of a file being processed by {knitr}. This is used with expect_output().\n\nhelper-snap-transform.R\n\nA function that is passed to the transform parameter of expect_snapshot() and will mask the temporary directory path so that the snapshot does not continuously invalidate on every run.\n\nhelper-translate.R\n\nDefines three expectations to test translations: expect_set_translated(), expect_title_translated() and expect_h1_translated(). All are documented in the helper file and in test-utils-translate.R\n\n\n\n\n7.2.2 Setup Script\nThe first script to run is tests/testthat/setup.R, where a test lesson and a local git remote is created and stored in a temporary location for the duration of the test suite and a reset function is exposed for the tests.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/testing.html#conditionally-skipped-tests",
    "href": "sandpaper/testing.html#conditionally-skipped-tests",
    "title": "7  Testing {sandpaper}",
    "section": "7.3 Conditionally Skipped Tests",
    "text": "7.3 Conditionally Skipped Tests\nEach link below will open a code search for the different types of skipped tests in {sandpaper}\n\nskip_on_os\n\nThese are often tests that are skipped on Windows, usually for the reason that {renv} and filepaths behave slightly differently on Windows in a continuous integration setting.\n\nskip_if\n\nTests that are skipped under various conditions. For example, if Git is not installed on a system, we should not test any of the CI functions because they rely on Git.\n\nskip(\"&lt;for reasons&gt;\")\n\nThis pattern is more rare. It’s really useful in a situation where you are refactoring and know that a lot of tests will fail. If you sprinkle in these skips, you can focus on testing the core functionality of the refactor and then address the side-effects. Regarding the skips that remain: during testing, sometimes we encounter a ghost in the machine and we cannot set up the right conditions to run the test properly or the test was created with an earlier model of the package that we haven’t been able to shake. In these cases, instead of deleting the test or commenting out code, we add the skip() function and write a message of why we skipped it so if we need to come back to it later, we can.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/testing.html#continuous-integration",
    "href": "sandpaper/testing.html#continuous-integration",
    "title": "7  Testing {sandpaper}",
    "section": "7.4 Continuous Integration",
    "text": "7.4 Continuous Integration\n\n7.4.1 Package Cache\nRunning tests on Continuous Integration is tricky in part because we need to set up a {renv} package cache to work on Mac, Windows, and Linux systems. In practise, we have to set up a specific RENV_PATHS_ROOT folder for each system.\n\n7.4.1.1 Windows\nFor Windows, the setup is even more complex because there are weird caveats in how pandoc and {renv} work on the CI version of Windows.\n\n\n\n7.4.2 Dependencies\nAt the moment, we test the current versions of dependencies when we are running tests in the test-coverage.yaml file. For the R-CMD-check.yaml file, however, we test the development version of {renv}. The reason why we do this is because in March 2023, {renv} 0.17.0 was released and subsequently broke bioconductor-based R Markdown lessons and new R Markdown lessons that needed to be bootstrapped (see sandpaper#406).\nThis can lead to a situation where the tests will pass on the test coverage check, but fail for R CMD check, which is diagnostic because it tells us that there is an upstream issue in {renv} that we can address before it becomes a problem after a CRAN release.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Testing {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/global-state.html",
    "href": "sandpaper/global-state.html",
    "title": "8  The global state in {sandpaper}",
    "section": "",
    "text": "8.1 When {sandpaper} is loaded\nWhen {sandpaper} is loaded (either by attatchment with library() or invocation with ::, a function called .onLoad() (which lives in R/zzz.R) is evaluated:\nsandpaper:::.onLoad\n\nfunction (libname, pkgname) \n{\n    ns &lt;- asNamespace(pkgname)\n    delayedAssign(\"GITIGNORED\", gitignore_items(), eval.env = ns, \n        assign.env = ns)\n    op &lt;- getOption(\"sandpaper.use_renv\")\n    if (is.null(op)) {\n        try_use_renv()\n    }\n    establish_translation_vars()\n    invisible()\n}\n&lt;bytecode: 0x557b805b4cf0&gt;\n&lt;environment: namespace:sandpaper&gt;\nThis does a few things:",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The global state in {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/global-state.html#when-sandpaper-is-loaded",
    "href": "sandpaper/global-state.html#when-sandpaper-is-loaded",
    "title": "8  The global state in {sandpaper}",
    "section": "",
    "text": "establishes a GITIGNORED vector that is used to check the .gitignore file to validate that we are working in a lesson that The Workbench can work with.\n\n\n\nGITIGNORED is a list of items required to be in the lesson’s .gitignore\n\n\nattempts to determine if the user has provided consent to use {renv} with try_use_renv(). This will set the internal sandpaper.use_renv environment variable.\nThe establish_translation_vars() will establish the internal list of translation strings inside the internal environment called these$translations for use with set_language(), which can be fetched with the tr_src(), and tr_get() functions. See establish_translation_vars() for details",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The global state in {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/global-state.html#building-the-lesson",
    "href": "sandpaper/global-state.html#building-the-lesson",
    "title": "8  The global state in {sandpaper}",
    "section": "8.2 Building the lesson",
    "text": "8.2 Building the lesson\nAll functions that build a lesson go through build_lesson():\n\nsandpaper::build_lesson\n\nfunction (path = \".\", rebuild = FALSE, quiet = !interactive(), \n    preview = TRUE, override = list()) \n{\n    check_pandoc()\n    slugpath &lt;- get_build_slug(path, quiet = quiet)\n    slug &lt;- slugpath$slug\n    path &lt;- slugpath$path\n    on.exit({\n        reset_build_paths()\n    })\n    validate_lesson(path, quiet = quiet)\n    built &lt;- build_markdown(path = path, rebuild = rebuild, quiet = quiet, \n        slug = slug)\n    build_site(path = path, quiet = quiet, preview = preview, \n        override = override, slug = slug, built = built)\n}\n&lt;bytecode: 0x557b83934f58&gt;\n&lt;environment: namespace:sandpaper&gt;\n\n\n\n8.2.1 Step 1: set an anchor\nBecause all of the lesson functions need to understand where the lesson exists on the file system, we want to record the root path while a lesson is being deployed. When build_lesson() is run, it calls the set_source_path() function, which records the root of the lesson (as determined by the presence of an “episodes” directory, OR a “site” directory, OR a “learners” directory, OR an “instructors” directory, OR a “profiles” directory) in a global envioronment called .build_paths$source:\n\nsandpaper:::set_source_path\n\nfunction (path) \n{\n    .build_paths$source &lt;- root_path(path) %||% .build_paths$source\n    invisible(.build_paths$source)\n}\n&lt;bytecode: 0x557b841eb608&gt;\n&lt;environment: namespace:sandpaper&gt;\n\n\n\n\ntime elapsed: 10.00732 secs\n\n\n\n\n\nset_source_path() establishes the root of the lesson\n\n\nWhen the function exits, the reset_build_paths() function is called to reset the build path to NULL so that it could be used for a new lesson.\n\nsandpaper:::reset_build_paths\n\nfunction () \n{\n    .build_paths$source &lt;- NULL\n}\n&lt;bytecode: 0x557b823d4808&gt;\n&lt;environment: namespace:sandpaper&gt;\n\n\n\n\ntime elapsed: 1.137902 secs\n\n\n\n\n\nreset_build_paths() establishes the root of the lesson\n\n\n\n\n8.2.2 Step 2: Validate and setup global variables\nWhen validate_lesson() is called, it kicks off a cascade that stores lesson-specific variables and strings for {varnish} into the language of the lesson. This process is described in The Data Flow vingette for {sandpaper}",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The global state in {sandpaper}</span>"
    ]
  },
  {
    "objectID": "sandpaper/package-cache.html",
    "href": "sandpaper/package-cache.html",
    "title": "9  The R package cache",
    "section": "",
    "text": "9.1 Introduction\nAll lessons that use The Workbench can build using either Markdown or R Markdown file formats. The R package cache allows for R Markdown file formats to be built reproducibly and consistently. The cache is expected to be mindful of these formats in the following ways (as taken from Building Lessons With A Package Cache):\nThe package cache is only used when building lessons with R Markdown elements and performs the folowing tasks in a separate R session to avoid polluting the user’s environment:\nThis chapter will go into the history of using a package cache in Carpentries lessons, dig into the design principles, and understand challenges for moving forward.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The R package cache</span>"
    ]
  },
  {
    "objectID": "sandpaper/package-cache.html#introduction",
    "href": "sandpaper/package-cache.html#introduction",
    "title": "9  The R package cache",
    "section": "",
    "text": "reliable setup: the version of the lesson built on the carpentries website will be the same as what you build on your computer because the packages will be identical\nenvironmentally friendly: The lesson dependencies are NOT stored in your default R library and they will not alter your R environment.\ntransparent: any additions or deletions to the cache will be recorded in the lockfile, which is tracked by git.\n\n\n\nBefore markdown is built: We check for consent to use the package cache with sandpaper:::renv_check_consent() and then provision any packages needed for the lesson with sandpaper::manage_deps()\n\n\n\n\n\n\nDependency Tree for renv_check_consent()\n\n\n\n\n\nThe renv_check_consent() checks if the user has run sandpaper::use_package_cache(), which allows {renv} to create and maintain a global package cache on their system.\n\n\n\n\nDuring each markdown rendering: If we have consent, we load the {renv} profile to reproducibly render the markdown document\n\n\n\n\n\n\nFlow Diagram for build_episode_md()\n\n\n\n\n\nThis flow represents the build process for each R Markdown file. This function is called from within a separate R process. The only conditional here determines if the {renv} environment needs to be loaded.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The R package cache</span>"
    ]
  },
  {
    "objectID": "sandpaper/package-cache.html#a-bit-of-history",
    "href": "sandpaper/package-cache.html#a-bit-of-history",
    "title": "9  The R package cache",
    "section": "9.2 A Bit of History",
    "text": "9.2 A Bit of History\nAs of this writing, The Workbench is able to reproducibly build and deploy lessons across machines and infrastructures thanks to the package cache provided by {renv}, but it is important to understand how we got here and what the motivations were, because the tools we have now simply did not exist in 2016. This section dives a bit into the history of writing R markdown content in Carpentries lessons and what lessons… were learned.\nSoftware Carpentry Lessons have been able to handle content written in R Markdown since July 2014. This process was disrupted in June 2016 with the release of the new styles template, but luckily, François Michonneau swooped in to the rescue by providing a templating setup that would not only render the Markdown, but keep the output separate. The very next month, François submitted carpentries/styles#83, which added the capability to automatically detect and install packages needed to build an R-based lesson.\nOf course, back in 2016, in order to deploy an R-based lesson, you still had to build it locally, which sounds simple until you consider the aspect of reproducibility (Marwick 2016; Wilson et al. 2017). If you build the same document on two different machines, there is no guarantee that you will get the same results. Thus, in May 2018, Raniere Silva added the ability to build R-based lessones on Continuous Integration. Finally, in 2020, Maxim Belkin added GitHub Workflows to the styles repository so that we no longer had to rely on TravisCI.\nThese changes allowed a single, definitive source for lessons to be built, but alas, they still were not reproducible because the packages used to build the lesson were always being run with the most recent versions. This lead to problems with outputs changing or worse, the entire build failing (see swcarpentry/r-novice-gapminder#746. Moreover, lesson maintainers of thes R lessons encountered the following problems:\n\nEvery time they built their lessons locally, their default R package library would update. This was especially a problem for maintainers who were working on their dissertations and really could not afford to lose work due to their packages changing.\nMaintainers were unsure of what would happen to the lesson with any given pull request and would have to manually run the results or trust the contributor.\nAll the normal struggles with Jekyll.\n\nReproducibility is hard. Software ecosystems are always shifting like the sand dunes in the Sahara (Vaniea and Rashidi 2016). By late 2020, we had come a long way in terms of automating the build process for R-based lessons, but there were still many hills to climb. It was in this context that we developed the use of {renv} and the R package cache to automate package provisioning, caching, and auditable updating so that R-based lessons could reliably be deployed with no surprises.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The R package cache</span>"
    ]
  },
  {
    "objectID": "sandpaper/package-cache.html#design-principles",
    "href": "sandpaper/package-cache.html#design-principles",
    "title": "9  The R package cache",
    "section": "9.3 Design Principles",
    "text": "9.3 Design Principles\nIt is worth reading through carpentries/sandpaper#21 to see the discussion and thoughts around the origin of the design for using this feature. It was implemented during a three week period between 2021-08-24 and 2021-09-16, as detailed in the pull request carpentries/sandpaper#158.\nIt’s easy to think about a package cache as a way to declare dependencies for a lesson in a reproducible way, but it’s more than that. The philosophy of R packages on CRAN is that they all need to work with the latest versions of each other, so a lesson that teaches R should be reasonably up-to-date. Thus, any given package must be able to do the following four things in the package cache:\n\nenter the package cache and record the version number\nexit the package cache and be removed from the record\nupdate to the latest version\npin to a specific version\n\nEvery good tool for handling a package cache does these four things, and the workflow to add a new package with {renv} (before version 1.0.0) flows like this:\n\nContributor A would like to add {packageA} to the project\nContributor A opens the project and installs {packageA}\nContributor A adds {packageA} to the project content\nContributor A takes a snapshot of the project to the lockfile\n\nIn this scenario, Contributor A must explicitly install the package to the project before they can use it regardless of whether or not they have that package on their system. Steps 2 and 4 of this workflow involve explicitly working with the package cache and, in my experience, R users will often jump directly to step 3, which leads to failed builds and frustration.\nWhen designing a user interface, you also need to think about what distractions and real-life stressors the lesson maintainer/developer/contributor is potentially dealing with. You want to minimize the amount of fuss that the contributor needs to do to get something working. A contributor should only have to add a package once to the lesson content to make it available. This means that the following steps are taken to add a new package:\n\nContributor A has {packageA} in their library and wants to demonstrate the use of {packageA} in the lesson.\nContributor A inserts a code block in the lesson that declares library(\"packageA\") or packageA::fun() somewhere in the code block:\n\n```{r}\n# load the packageA package\nlibrary(\"packageA\")\n```\n\nAfter that, when the lesson is built, {packageA} is automatically added to the lockfile. If Contributor A decideds to no longer use {packageA}, they can remove it from the lesson content and the next build will remove {packageA} from the lockfile. In this way, the lesson content remains the source of truth for the packages used in the lesson and the lockfile represents the metadata associated with the lesson.\nThat means the following WRT to packages in a lesson:\n\nUsers should not have to know that the lockfile exists\nThe packages used in the lesson should be locked to specific versions and be reproducible across machines\nThe packages used in the lesson should not overwrite the packages in the user’s default R library\nAny package that is missing from the user’s machine should automatically be provisioned\nAll packages used should be the correct version\nThe lockfile should be auditable\nThe lockfile defining the package versions should update/remove packages according to the contents of the lesson\nUsers should be able to specify versions in the lockfile easily\nUsers should be able to automatically update the lockfile\n\n\n\n\n\nMarwick, Ben. 2016. “Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation.” Journal of Archaeological Method and Theory 24 (2): 424–50. https://doi.org/10.1007/s10816-015-9272-9.\n\n\nVaniea, Kami, and Yasmeen Rashidi. 2016. “Tales of Software Updates.” Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, May. https://doi.org/10.1145/2858036.2858303.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” Edited by Francis Ouellette. PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The R package cache</span>"
    ]
  },
  {
    "objectID": "sandpaper/building-html.html",
    "href": "sandpaper/building-html.html",
    "title": "10  Building HTML",
    "section": "",
    "text": "10.1 Introduction\nBuilding HTML in The Workbench is a process that is isolated from rendering R Markdown to Markdown. There are three general steps to building the HTML files:",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building HTML</span>"
    ]
  },
  {
    "objectID": "sandpaper/building-html.html#introduction",
    "href": "sandpaper/building-html.html#introduction",
    "title": "10  Building HTML",
    "section": "",
    "text": "render the HTML using pandoc with our custom lua filters (which transform our special fenced-divs and flattens relative links)\npost-process the HTML using {xml2} to further refine and translate elements that we cannot achieve through Lua filters. It is here that we do things like adding heading anchors, removing instructor notes for learner view, etc.\napply {varnish} templates by passing global variables and HTML to {pkgdown}. This is where the metadata and most of the translations are applied.\n\n\n10.1.1 Page types in The Workbench\nThere are generally three kinds of pages that occur in The Workbench: single content pages where the HTML is directly generated from the Markdown, combined pages where two or more Markdown files contribute to the page content, and aggregate pages where the HTML is collected from specific sections of the single content pages after they have been rendered.\n\n10.1.1.1 Single Content Pages\nThese pages are generated directly from Markdown content and are going to be in the episodes/, instructors/, and learners/ folders. There are special exceptions that are used for combined pages and aggregate pages.\n\n\n10.1.1.2 Combined Pages\nCombined pages are rendered to HTML and then concatenated together.\n\n10.1.1.2.1 index.md and learners/setup.md\nThese form /index.html aka the home page where the sidebar shows the sections of the setup document.\n\n\n\nsource\nsection\nanchor\n\n\n\n\nindex.md\nindex\n#\n\n\nlearners/setup.md\nsetup\n#setup\n\n\n\nThe instructor view is similar, except it includes a schedule before the setup:\n\n\n\nsource\nsection\nanchor\n\n\n\n\nindex.md\nindex\n#\n\n\nepisodes/*\nschedule\n#schedule\n\n\nlearners/setup.md\nsetup\n#setup\n\n\n\n\n\n10.1.1.2.2 profiles/*\nAll the profiles pages are concatenated into a single page called /profiles.html.\n\n\n10.1.1.2.3 instructors/instructor-notes.md and episodes/*\nThe instructor notes (global and inline) are in a page called /instructor/instructor-notes.html The instructor notes are a special kind of combination that adds the in-line instructor notes to the end of the instructor-notes.md file. The in-line instructor notes start at the section with the anchor #aggregate-instructor-notes with subsections that have the anchor of the episode filenames.\n\n\n\n10.1.1.3 Aggregate Pages\n\n10.1.1.3.1 All in one page\nThis is at /aio.html and is a concatenation of the schedule. Each episode will be in a section with the ID aio-&lt;file-slug&gt;. An episode file named introduction.md will have the address /aio.html#aio-introduction in the aio page. Note: the aio prefix is to prevent a clash with the ID.\n\n\n10.1.1.3.2 Keypoints\nThis is at /key-points.html. The key points of each episode is extracted and placed into sections with the ID of &lt;file-slug&gt;. An episode file name introduction.md will have the address /key-points.html#introduction.\n\n\n10.1.1.3.3 Images\nThis is at /instructor/images.html and contains a concatenation of all the images in the lesson, printing the alt-text for each image if it exists.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building HTML</span>"
    ]
  },
  {
    "objectID": "sandpaper/building-html.html#rendering-html",
    "href": "sandpaper/building-html.html#rendering-html",
    "title": "10  Building HTML",
    "section": "10.2 Rendering HTML",
    "text": "10.2 Rendering HTML\nThe first step of the pipeline is to render the HTML from Markdown (in the site/built folder) using pandoc with our custom lua filters. This is entirely encapsulated within the function render_html():\n\n\n\n\n\nflowchart TB\n    IN[\"site/built/FILE.md\"]\n    TMP[\"/tmp/tempfile.html\"]\n    OUT[/\"R Object\"\\]\n    pandoc{{\"pandoc\"}}\n    LUA[\"sandpaper/../lesson.lua\"]\n\n    IN --&gt; pandoc --&gt; TMP\n    TMP --&gt;|\"readLines()\"| OUT\n    LUA --&gt;|\"construct_pandoc_args()\"| pandoc\n\n\n\n\n\n\nrender_html() is called by all of the pages which need to render content from markdown:\n\n\n\nrender_html() generates HTML from Markdown files\n\n\nThe HTML that is rendered from render_html() is not standalone and expected to be insert into the &lt;main&gt; tags of HTML boilderplate. Luckily, render_html() is a standalone function. This means that you do not need to prepare anything other than a document for it to render. Here’s an example rendering basic Markdown to HTML as a character string. Let’s say we had a single markdown document that just showed:\nI can write in **Markdown** and $\\\\LaTeX$ :joy_cat:\nWhen rendered, it would look like this:\n\nI can write in Markdown and \\(\\\\LaTeX\\) 😹\n\nTo test the rendering, we can create a temporary file on the fly and use render_html() to render it.\n\ntmp &lt;- tempfile()\nwriteLines(\"I can write in **Markdown** and $\\\\LaTeX$ :joy_cat:\", tmp)\nhtml_text &lt;- sandpaper:::render_html(tmp)\n\nThe html_text variable contains a string of length 1 that contains the rendered HTML from the markdown.\n&lt;p&gt;I can write in &lt;strong&gt;Markdown&lt;/strong&gt; and &lt;span\nclass=\"math inline\"&gt;\\(\\LaTeX\\)&lt;/span&gt; &lt;span class=\"emoji\"\ndata-emoji=\"joy_cat\"&gt;😹&lt;/span&gt;&lt;/p&gt;\n\n10.2.1 Working with the Output\nfor all of the functions that use render_html() you will find this general pipeline: file_path |&gt; render_html() |&gt; xml2::read_html().\n\nhtml_text &lt;- render_html(file_path)\nif (html_text == \"\") {\n  html &lt;- xml2::read_html(\"&lt;p&gt;&lt;/p&gt;\")\n} else {\n  html &lt;- xml2::read_html(html_text)\n}\n\nYou can then use it to explore and manipulate the elements using good ol’ XPath synatax 🤠 Yee haw!\n\n\n\n\n\n\n✋ Wait just a rootin’ tootin’ minute!\n\n\n\n\n😩 We have HTML, why are we using XML to parse it?\n🤠 Well, pardner, just like cowpolk can rustle up cows, sheep, goats, and even cats, XPath is a language that can be used to rustle up ANY sort of pointy-syntax markup like HTML, XML, SVG, and even CSL.\n😲 That’s a good point!\n🤠 Fastest pun in the West!\n😉\n\n\n\n\nprint(html)\n\n## {html_document}\n## &lt;html&gt;\n## [1] &lt;body&gt;&lt;p&gt;I can write in &lt;strong&gt;Markdown&lt;/strong&gt; and &lt;span class=\"math i ...\n\nxml2::xml_find_all(html, \".//p/strong\")\n\n## {xml_nodeset (1)}\n## [1] &lt;strong&gt;Markdown&lt;/strong&gt;\n\nxml2::xml_find_all(html, \".//p/span[@class='emoji']\")\n\n## {xml_nodeset (1)}\n## [1] &lt;span class=\"emoji\" data-emoji=\"joy_cat\"&gt;😹&lt;/span&gt;\n\n\nThe HTML can also be copied by converting it to a character and re-reading it as XML (yes, this is legitimately the fastest way to do this).\n\n\n\n\n\n\nNote\n\n\n\nSee the {pegboard} intro to XML about the memory of XML objects for a reason why you want to copy XML documents this way.\n\n\n\nhtml2 &lt;- xml2::read_html(as.character(html))\n\nFrom here, the nodes get sent to fix_nodes() so that they can be post-processed.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building HTML</span>"
    ]
  },
  {
    "objectID": "sandpaper/building-html.html#post-processing-with-xpath",
    "href": "sandpaper/building-html.html#post-processing-with-xpath",
    "title": "10  Building HTML",
    "section": "10.3 Post-processing with XPath",
    "text": "10.3 Post-processing with XPath\nBefore the HTML can be passed to the template, it needs to be tweaked a bit. There are two reasons why we would need to tweak the HTML:\n\nWe want to add a feature that is not supported in pandoc (or at least older versions)\nWe need to structurally rearrange pandoc defaults to match our template\n\nTo do this, we read in the HTML with xml2::read_html() and then manipulate it using the sandpaper internal function fix_nodes(), which is called by the following functions:\n\n\n\nfix_nodes() depends on build_site()\n\n\nIn turn, fix_nodes() will call this cascade of XML manipulating functions:\n\nsandpaper:::fix_nodes\n\nfunction (nodes = NULL) \n{\n    if (length(nodes) == 0) \n        return(nodes)\n    translate_overview(nodes)\n    fix_headings(nodes)\n    fix_accordions(nodes)\n    fix_callouts(nodes)\n    fix_codeblocks(nodes)\n    fix_figures(nodes)\n    fix_setup_link(nodes)\n}\n&lt;bytecode: 0x556b8d9e5fa0&gt;\n&lt;environment: namespace:sandpaper&gt;\n\n\n\n\n\nCall tree for fix_nodes()\n\n\nI will show the effects of each of these functions one by one, but first, here’s a couple of functions that will help me demonstrate so I don’t have to keep retyping an copying/pasting:\n\nrender_and_parse &lt;- function(txt) {\n  tmp &lt;- tempfile()\n  writeLines(txt, tmp)\n  return(xml2::read_html(sandpaper:::render_html(tmp)))\n}\n\nprint_html &lt;- function(html, xpath = \".//body/*\") {\n  writeLines(as.character(xml2::xml_find_all(html, xpath)))\n}\n\n\n10.3.1 translate_overview()\nThe Overview card is produced by the Lua filter, combining the objectives and questions fenced divs into one entity:\n\novr &lt;- \"\n::: objectives\n\n- one\n\n:::\n::: questions\n\n- one?\n\n:::\"\n\nrender_and_parse(ovr) |&gt; \n  print_html()\n\n&lt;div class=\"overview card\"&gt;\n&lt;h2 class=\"card-header\"&gt;Overview&lt;/h2&gt;\n&lt;div class=\"row g-0\"&gt;\n&lt;div class=\"col-md-4\"&gt;\n&lt;div class=\"card-body\"&gt;\n&lt;div class=\"inner\"&gt;\n&lt;h3 class=\"card-title\"&gt;Questions&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;one?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"col-md-8\"&gt;\n&lt;div class=\"card-body\"&gt;\n&lt;div class=\"inner bordered\"&gt;\n&lt;h3 class=\"card-title\"&gt;Objectives&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;one&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\n\nThe only purpose for translate_overview() is to translate the entities for this card into different languages, so if we use it in an English context, nothing happens, but if we use it in a Japanese context, the the translation appears:\n\nwithr::with_language(\"ja\", {\n  render_and_parse(ovr) |&gt; \n    sandpaper:::translate_overview() |&gt; \n    print_html()\n})\n\n&lt;div class=\"overview card\"&gt;\n&lt;h2 class=\"card-header\"&gt;Overview&lt;/h2&gt;\n&lt;div class=\"row g-0\"&gt;\n&lt;div class=\"col-md-4\"&gt;\n&lt;div class=\"card-body\"&gt;\n&lt;div class=\"inner\"&gt;\n&lt;h3 class=\"card-title\"&gt;Questions&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;one?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"col-md-8\"&gt;\n&lt;div class=\"card-body\"&gt;\n&lt;div class=\"inner bordered\"&gt;\n&lt;h3 class=\"card-title\"&gt;Objectives&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;one&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\n\n\n\n10.3.2 fix_headings()\nHeadings in The Workbench need a couple of things done:\n\nThe parent div needs to be a &lt;section&gt; tag\nThey need anchor links added to the headings (we took ours from {pkdown})\nThe section element needed to be a section-heading class and there needed to be an &lt;hr&gt; element underneath the &lt;h2&gt; tags.\n\nBehold!\n\nheads &lt;- \"\n## Heading 2 {#i-am-a-section-link}\n\n### Heading 3 \n\nThis is in a section, but it's not a section\n\n## Heading 2\n\nThis is a new section\n\"\nrender_and_parse(heads) |&gt; print_html()\n\n&lt;div id=\"i-am-a-section-link\" class=\"section level2\"&gt;\n&lt;h2&gt;Heading 2&lt;/h2&gt;\n&lt;div id=\"heading-3\" class=\"section level3\"&gt;\n&lt;h3&gt;Heading 3&lt;/h3&gt;\n&lt;p&gt;This is in a section, but it’s not a section&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div id=\"heading-2\" class=\"section level2\"&gt;\n&lt;h2&gt;Heading 2&lt;/h2&gt;\n&lt;p&gt;This is a new section&lt;/p&gt;\n&lt;/div&gt;\n\nrender_and_parse(heads) |&gt; sandpaper:::fix_headings() |&gt; print_html()\n\n&lt;section id=\"i-am-a-section-link\"&gt;\n&lt;h2 class=\"section-heading\"&gt;Heading 2&lt;a class=\"anchor\" aria-label=\"anchor\" href=\"#i-am-a-section-link\"&gt;&lt;/a&gt;\n&lt;/h2&gt;\n&lt;hr class=\"half-width\"&gt;\n&lt;div id=\"heading-3\" class=\"section level3\"&gt;\n&lt;h3&gt;Heading 3&lt;/h3&gt;\n&lt;p&gt;This is in a section, but it’s not a section&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n&lt;section id=\"heading-2\"&gt;\n&lt;h2 class=\"section-heading\"&gt;Heading 2&lt;a class=\"anchor\" aria-label=\"anchor\" href=\"#heading-2\"&gt;&lt;/a&gt;\n&lt;/h2&gt;\n&lt;hr class=\"half-width\"&gt;\n&lt;p&gt;This is a new section&lt;/p&gt;\n&lt;/section&gt;\n\n\n\n\n10.3.3 fix_accordions()\nThe only thing that happens with accordions is that they get translated:\n\naccord &lt;- \"\n::: instructor\n\ndrink water\n\n:::\n\"\nrender_and_parse(accord) |&gt; print_html(\".//h3/text()\")\n\n\n  \n\n  Instructor Note\n  \n\nwithr::with_language(\"ja\", {\n  render_and_parse(accord) |&gt; \n    sandpaper:::fix_accordions() |&gt; \n    print_html(\".//h3/text()\")\n})\n\n\nInstructor Note\n\n\n\n\n10.3.4 fix_callouts()\nCallouts need to have translations applied and ids adjusted:\n\nkeyps &lt;- \"\n::: keypoints\n\n - hydrate\n\n:::\"\nrender_and_parse(keyps) |&gt; print_html()\n\n&lt;div id=\"keypoints1\" class=\"callout keypoints\"&gt;\n&lt;div class=\"callout-square\"&gt;\n&lt;i class=\"callout-icon\" data-feather=\"key\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"section level3 callout-title callout-inner\"&gt;\n&lt;h3 class=\"callout-title\"&gt;Keypoints&lt;/h3&gt;\n&lt;div class=\"callout-content\"&gt;\n&lt;ul&gt;\n&lt;li&gt;hydrate&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\nrender_and_parse(keyps) |&gt; sandpaper:::fix_callouts() |&gt; print_html()\n\n&lt;div id=\"keypoints1\" class=\"callout keypoints\"&gt;\n&lt;div class=\"callout-square\"&gt;\n&lt;i class=\"callout-icon\" data-feather=\"key\"&gt;&lt;/i&gt;\n&lt;/div&gt;\n&lt;div class=\"callout-inner\"&gt;\n&lt;h3 class=\"callout-title\"&gt;Key Points&lt;a class=\"anchor\" aria-label=\"anchor\" href=\"#keypoints1\"&gt;&lt;/a&gt;\n&lt;/h3&gt;\n&lt;div class=\"callout-content\"&gt;\n&lt;ul&gt;\n&lt;li&gt;hydrate&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\n\nThe translations are also applied:\n\nwithr::with_language(\"ja\", {\n  render_and_parse(keyps) |&gt; \n    sandpaper:::fix_callouts() |&gt; \n    print_html(\".//h3/text()\")\n})\n\nKey Points\n\n\n\n\n10.3.5 fix_codeblocks()\nCodeblocks have a phantom H3 attached\n\ncodes &lt;- \"\n```r\ncat('mouse')\n```\"\nrender_and_parse(codes) |&gt; print_html()\n\n&lt;div class=\"sourceCode\" id=\"cb1\"&gt;&lt;pre class=\"sourceCode r\"&gt;&lt;code class=\"sourceCode r\"&gt;&lt;span id=\"cb1-1\"&gt;&lt;a href=\"#cb1-1\" tabindex=\"-1\"&gt;&lt;/a&gt;&lt;span class=\"fu\"&gt;cat&lt;/span&gt;(&lt;span class=\"st\"&gt;'mouse'&lt;/span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;\n\nrender_and_parse(codes) |&gt; sandpaper:::fix_codeblocks() |&gt; print_html()\n\n&lt;div class=\"codewrapper sourceCode\" id=\"cb1\"&gt;\n&lt;h3 class=\"code-label\"&gt;R&lt;i aria-hidden=\"true\" data-feather=\"chevron-left\"&gt;&lt;/i&gt;&lt;i aria-hidden=\"true\" data-feather=\"chevron-right\"&gt;&lt;/i&gt;\n&lt;/h3&gt;\n&lt;pre class=\"sourceCode r\" tabindex=\"0\"&gt;&lt;code class=\"sourceCode r\"&gt;&lt;span id=\"cb1-1\"&gt;&lt;a href=\"#cb1-1\" tabindex=\"-1\"&gt;&lt;/a&gt;&lt;span class=\"fu\"&gt;cat&lt;/span&gt;(&lt;span class=\"st\"&gt;'mouse'&lt;/span&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;\n\n\n\n\n10.3.6 fix_figures()\nFigures need semantic HTML &lt;figure&gt;, not &lt;div&gt; and lone images with no captions should still be figures.\n\nfigs &lt;- \"\n![](lone-img.png){alt='tumbleweed'}\n\n![Just Sayin', Breakfast Plums, So Cold](papa-eating-plums.png){alt='an empty icebox, plums gone'}\n\"\nrender_and_parse(figs) |&gt; print_html()\n\n&lt;p&gt;&lt;img src=\"lone-img.png\" alt=\"tumbleweed\"&gt;&lt;/p&gt;\n&lt;div class=\"float\"&gt;\n&lt;img src=\"papa-eating-plums.png\" alt=\"an empty icebox, plums gone\"&gt;&lt;div class=\"figcaption\"&gt;Just Sayin’, Breakfast Plums, So Cold&lt;/div&gt;\n&lt;/div&gt;\n\nrender_and_parse(figs) |&gt; sandpaper:::fix_figures() |&gt; print_html()\n\n&lt;figure&gt;&lt;img src=\"lone-img.png\" alt=\"tumbleweed\" class=\"figure mx-auto d-block\"&gt;&lt;/figure&gt;\n&lt;figure&gt;\n&lt;img src=\"papa-eating-plums.png\" alt=\"an empty icebox, plums gone\" class=\"figure mx-auto d-block\"&gt;&lt;div class=\"figcaption\"&gt;Just Sayin’, Breakfast Plums, So Cold&lt;/div&gt;\n&lt;/figure&gt;\n\n\n\n\n10.3.7 fix_setup_link()\nWhen someone writes the link to setup.html, it needs to be transmogrified to be index.html#setup\n\nstp &lt;- \"\n[setup](../learners/setup.html) \n\nwith [macOS](../learners/setup.html#macos)\"\n\nrender_and_parse(stp) |&gt; print_html()\n\n&lt;p&gt;&lt;a href=\"setup.html\"&gt;setup&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;with &lt;a href=\"setup.html#macos\"&gt;macOS&lt;/a&gt;&lt;/p&gt;\n\nrender_and_parse(stp) |&gt; sandpaper:::fix_setup_link() |&gt; print_html()\n\n&lt;p&gt;&lt;a href=\"index.html#setup\"&gt;setup&lt;/a&gt;&lt;/p&gt;\n&lt;p&gt;with &lt;a href=\"index.html#macos\"&gt;macOS&lt;/a&gt;&lt;/p&gt;",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building HTML</span>"
    ]
  },
  {
    "objectID": "sandpaper/building-html.html#applying-varnish-templating-with-pkgdownrender_page",
    "href": "sandpaper/building-html.html#applying-varnish-templating-with-pkgdownrender_page",
    "title": "10  Building HTML",
    "section": "10.4 Applying {varnish} templating with pkgdown::render_page()",
    "text": "10.4 Applying {varnish} templating with pkgdown::render_page()\nAll HTML files get funneled into pkgdown::render_page() through build_html(), which in turn is ultimately called by build_site().\n\n\n\n\n\n\nWant to change the HTML engine?\n\n\n\nIf we were to change the templating engine (e.g. Quarto, Hugo, or (ugh) Jekyll1), this is the place to change it. Of course, it will not be ‘easy’, but it will be possible.\n\n\n\n\n\npkgdown::render_page() is the last stop from build_site()\n\n\nWhat you can see from the diagram, this function is the last step in the line to generate a website. As mentioned earlier, this will apply {varnish} templates to the HTML generated by render_html() and modified by fix_nodes(). These templates contain mustache template variables that need to be filled in with variables passed through the data argument in pkgdown::render_page(). These variables are generated when build_lesson() is called and are derived from both the config.yaml and episode metadata. Thus, it is important to read the Data Flow from Source to Website vignette in {sandpaper} to understand how the flow works.",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building HTML</span>"
    ]
  },
  {
    "objectID": "sandpaper/building-html.html#footnotes",
    "href": "sandpaper/building-html.html#footnotes",
    "title": "10  Building HTML",
    "section": "",
    "text": "Actually, one of the strengths of Jekyll is its ability to template websites by stitching together HTML. Just please, never expect anyone else to be able to render markdown with it or build from it locally.↩︎",
    "crumbs": [
      "{sandpaper} User Interface",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building HTML</span>"
    ]
  },
  {
    "objectID": "pegboard/intro.html",
    "href": "pegboard/intro.html",
    "title": "11  The {pegboard} package",
    "section": "",
    "text": "11.1 Introduction\nThe {pegboard} package was the very first package written for The Carpentries Workbench. It’s initial purpose was to parse the lessons based on the styles lesson infrastructure to figure out how lesson authors and maintainers were using the challenges and solutions. You can see this analysis in a vignette I started in May 2020 1.\nIt’s purpose now is two-fold:",
    "crumbs": [
      "{pegboard} Validation and Parsing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The {pegboard} package</span>"
    ]
  },
  {
    "objectID": "pegboard/intro.html#introduction",
    "href": "pegboard/intro.html#introduction",
    "title": "11  The {pegboard} package",
    "section": "",
    "text": "parse and validate the lessons for structural markdown elements\ntranslate Carpentries-style materials from the styles lesson infrastructure (Jekyll-based) to The Workbench (Pandoc-based)\n\n\n11.1.1 Dependencies\nThe dependency footprint of {pegboard} is intended to be small. The package is intended for validation and translation. It is meant to be stable.\nPegboard was built on top of the {tinkr} package, intially developed by Maëlle Salmon (rOpenSci) and now maintained by Zhian Kamvar. This package uses the CommonMark C library to parse Markdown into XML and then uses a custom XSLT stylesheet to translate Markdown back to XML. One of the key advantages that CommonMark’s XML gives us is the ability to extract line numbers and positions for markdown elements, which allows us to accurately report any Markdown issues to the user.\nThe objects used in pegboard are created with the {R6} package, which implements encapsulated object orientated programming for R. This style of programming is very similar to that of Python or Java. One of the reasons why we use {R6} is because objects created in this system are modified in place by their methods. This is important because the package to manipulate XML data, {xml2}, is built directly on top of the libxml2 C library, which also modifies objects in place, but it’s not inherently obvious when you work with them, so having a formal system like {R6} to encapsulate them makes more sense than a functional programming framework.",
    "crumbs": [
      "{pegboard} Validation and Parsing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The {pegboard} package</span>"
    ]
  },
  {
    "objectID": "pegboard/intro.html#footnotes",
    "href": "pegboard/intro.html#footnotes",
    "title": "11  The {pegboard} package",
    "section": "",
    "text": "This vignette no longer exists. It stopped working in November 2021, because of updates to the dependencies and the lessons. The vingette took a few minutes to build because it needed to download the lessons, and it was no longer appropriate as {pegboard} was shaping up to be the lesson validator and translator.↩︎",
    "crumbs": [
      "{pegboard} Validation and Parsing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The {pegboard} package</span>"
    ]
  },
  {
    "objectID": "varnish/intro.html",
    "href": "varnish/intro.html",
    "title": "12  The {varnish} package",
    "section": "",
    "text": "12.1 Introduction\nThe {varnish} package is a weird little package in that it does not contain any actual R code. It’s purpose is to host HTML templates along with the CSS and JavaScript needed to display the lesson.\nWe take advantage of the fact that the only thing actually required to install an R package is the presence of a DESCRIPTION file1. These all live inside the inst/ folder, which is a place that allows files to be installed along with the package.\nThe {pkgdown} package uses this as a mechanism for package developers to override the default styling, creating customized documentation websites such as the rOpenSci documentation sites: https://docs.ropensci.org/rotemplate/.\nThis allows people to update the lesson styling however they wish2 and while we could include it in {sandpaper}, it’s best kept separate so that people can update {varnish} without needing to update the entire tool suite.",
    "crumbs": [
      "{varnish} Web Styling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The {varnish} package</span>"
    ]
  },
  {
    "objectID": "varnish/intro.html#design-and-implementation-background",
    "href": "varnish/intro.html#design-and-implementation-background",
    "title": "12  The {varnish} package",
    "section": "12.2 Design and Implementation Background",
    "text": "12.2 Design and Implementation Background\nThe design for the frontend was created by Emily de la Mettrie in 2021 after consultation with Zhian N. Kamvar and François Michonneau using examples from The Unix Shell and parts of Exploring Data Frames for content cues.\nThe final figma design project3 was then handed off to a team at Bytes.co, who translated the designs to CSS and JavaScript, subcontracted an a11y testing company to interactively test the prototype for a11y issues.\nThe prototype we recieved from Bytes.co was a Jekyll template serving HTML files. Zhian created a staging repository called shellac to transform the site from one that was served via static site generator to one that was standalone. The preview is preserved at https://zkamvar.github.io/shellac/student_carpentries.html.\nThis site was then stripped of the added into {varnish} in carpentries/varnish#14 between 2022-01-10 and 2022-01-24, when the 1.0.0 release of {varnish} was created and the sandpaper docs website was updated to use the new version of the HTML, CSS, and JavaScript.",
    "crumbs": [
      "{varnish} Web Styling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The {varnish} package</span>"
    ]
  },
  {
    "objectID": "varnish/intro.html#footnotes",
    "href": "varnish/intro.html#footnotes",
    "title": "12  The {varnish} package",
    "section": "",
    "text": "for example, a DESCRIPTION file is a low-rent way of specifying depedencies for a manuscript).↩︎\nThere is a caveat, though. There are some components, such as the sidebar and overview callouts that are built on the {sandpaper} side and then fed into the template.↩︎\nThe final figma design project for The Workbench can be found in this link. To preview what it looks like, click the “Present” button (iconized as a triangle play button) in the top right.↩︎",
    "crumbs": [
      "{varnish} Web Styling",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The {varnish} package</span>"
    ]
  },
  {
    "objectID": "releases.html",
    "href": "releases.html",
    "title": "13  Release Process for Workbench Packages",
    "section": "",
    "text": "13.1 Background\nThe workbench contains three main packages:\nEach of these packages are available on the Carpentries R-Universe and new versions are checked for hourly. This allows folks to get up-to-date versions of The Workbench packages built for their system without running out of GitHub API query attempts.\nIn order to maintain quality, packages are only sent to the R-Universe if they have been formally released on GitHub (as specified in the packages.json configuration file). This allows us to incrementally add new experimental features without changing the stable deployments.",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Release Process for Workbench Packages</span>"
    ]
  },
  {
    "objectID": "releases.html#background",
    "href": "releases.html#background",
    "title": "13  Release Process for Workbench Packages",
    "section": "",
    "text": "{sandpaper}: user interface and workflow engine\n{pegboard}: parsing and validation engine\n{varnish}: HTML templates, CSS, and JS elements",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Release Process for Workbench Packages</span>"
    ]
  },
  {
    "objectID": "releases.html#versioning",
    "href": "releases.html#versioning",
    "title": "13  Release Process for Workbench Packages",
    "section": "13.2 Versioning",
    "text": "13.2 Versioning\nThe workbench is built using very basic semantic versioning using the X.Y.Z[.9000] pattern.\n\nX\n\nMajor version number this version number will change if there are significant breaking changes to any of the user-facing workflows. That is, if a change requires users to modify their scripts, then it is a breaking change. For example: at this time of writing, {pegboard} is at version 0.7.0, for which the Lesson object assumes a Jekyll lesson by default. If we were to change this to assume a sandpaper lesson by default, then this would be a breaking change because anyone who is using Lesson$new() will now need to use Lesson$new(jekyll = TRUE).\n\nY\n\nMinor version number this version number will change if there are new features or enhanced behaviors available to the users in a way that does not affect how users who do not need the new features use the package. An example of this is {pegboard} version 0.7.0 introduced processing of child documents. If the lesson has no child documents, then it is processed as expected, but if the child documents exist, then they are additionally processed and validated.\n\nZ\n\nPatch version number this version number will change if something that was previously broken was fixed, but no new features have been added. An example of this is in {sandpaper} version 0.13.3 where there were two bugfixes that affected links to setup page sections and 404 page rendering that otherwise did not add any new features.\n\n9000\n\nDevelopment version indicator this indicates that the version of {sandpaper} on GitHub is in development and not yet released. This indicator is always appended to the current release number and should be between 9000 and 9999. For example, you the development version indicator for the version of sandpaper after 0.13.2 is 0.13.2.9000. This particular version resulted in a bump to 0.13.3, but it could also have resulted in 0.14.0 or 1.0.0. It has not been extensively used in the past, but it is quite useful for tracking incremental changes with different bugfixes and features that may appear. When this development version indicator exists, the documentation site will have an extra dev/ directory that contains the upcoming changes so that we can continue to develop the workbench without disrupting the regular documentation flow.",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Release Process for Workbench Packages</span>"
    ]
  },
  {
    "objectID": "releases.html#release-process",
    "href": "releases.html#release-process",
    "title": "13  Release Process for Workbench Packages",
    "section": "13.3 Release Process",
    "text": "13.3 Release Process\n\n\n\n\n\n\nNon-urgent releases only\n\n\n\nThis release process assumes that we have accumulated bugfixes and/or features in the main branch, which we are ready to release. If you have a bug that needs to be patched immediately and you have new features in the main branch that are not yet released, then you should create a hotfix instead.\n\n\nWhen a package is ready for release we use the following checklist:\n\nCreate a new branch from main called release-X.Y.Z\nUpdate version number in DESCRIPTION and check that the Remotes: is up-to-date\nAdd NEWS for the changes in this version\ncommit, push changes, and create pull request\ncheck the pull request against the workbench integration test\nMerge the pull request when checks pass\nadd new signed tag with the name “ X.Y.Z” and push\n# example: create a signed (-s) tag for sandpaper 3.3.3\ngit tag -s 3.3.3 -m '&lt;short explanation of what changed&gt;'\ngit push --tags\ncreate a release on github from the new tag\n\n\n\n\n\n\n\nNote\n\n\n\nZhian likes to create tags via the command line because he has set up his git configuration to use a gpg signature so the tags and the releases are both verified.\n\n\nThe last two items can be achieved in a single step with the github cli with the command gh release create X.Y.Z for the version number\ngh release create 3.3.3\n# ? Title (optional) sandpaper 3.3.3\n# ? Release notes  [Use arrows to move, type to filter]\n#   Write my own\n# &gt; Write using generated notes as template\n#   Leave blank\nSelecting “Write using generated notes as a template” opens an editor and populates it with the pull requests that have been accepted since the last release.\nOnce the release is created on GitHub, then the package will be available on the R-Universe in about an hour or less.",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Release Process for Workbench Packages</span>"
    ]
  },
  {
    "objectID": "releases.html#release-timeline",
    "href": "releases.html#release-timeline",
    "title": "13  Release Process for Workbench Packages",
    "section": "13.4 Release Timeline",
    "text": "13.4 Release Timeline\nIn the early days of The Workbench, the release process was ad-hoc because the number of users was low, but now that the entire community is using it, it is important to set expectations and properly communicate changes to the users. For patch relases, these releases should continue to happen ad-hoc because, by definition, these releases only fix bugs and do not add new features.\nThe release timelines are based on two assumptions:\n\na M–F workweek (i.e. NEVER release on a Friday unless you like working on the weekend)\nAll lesson builds and updates happen every Tuesday at 00:00 UTC (packages for R-based lessons are updated on the first Tuesday)\n\n\n13.4.1 Minor Feature Updates\nFor releases that add new non-breaking features which bump the minor version, it is important to alert the community a couple of days in advance of the release so they have a chance to give feedback before the release. Thus, I propose the following timeline:\n\nFriday (drafting): Members of Workbench Maintainer Group draft communications describing new features\nMonday (communications): Communications describing new features sent to relevant communities (maintainers/trainers/instructors, depending on feature)\nWednesday (release): New version of Workbench package(s) released\n\nWe release on Wednesday in order to give enough time for the communications to sink in and give enough time for us to release patches to the release before all lessons get rebuilt.\n\n\n13.4.2 Breaking Changes\nIf there are breaking changes, it is important to give the community enough time to adapt and to offer them a path forward to fix their workflows. Depending on how drastic the change is, it may be important to conduct beta testing to ensure that the change doesn’t contain any unforseen bugs and, importantly, that it improves the workflow for the audience over doing nothing at all.\nThe Beta Phase of The Workbench is an extreme version of this, but it is important, because major version changes have real-world consequences that you want to avoid as much as possible.\nThe release should still happen on a Wednesday, but the communications aspect should be timely, specific, targeted, and repeated. It may be that you have to communicate about the change one week, one month, or even one quarter ahead of the release. In a volunteer community, you will end up in situations where people are checked out for months at a time and will see everything changed, so please do take care with the communications and changes.",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Release Process for Workbench Packages</span>"
    ]
  },
  {
    "objectID": "hotfixes.html",
    "href": "hotfixes.html",
    "title": "14  Releasing Hotfixes",
    "section": "",
    "text": "14.1 Step 1: Create a Fix and Pull Request\nThen, you get a report (let’s call it issue number 143) that something is broken and you need to fix it immediately. Normally, to fix bugs, you would check out a new bugfix branch from the main branch. Here’s the catch: if you check out from the main branch you will grab feature1 as well, which is not yet ready for production because you intended feature2 to be released along side. In this case, you need to create a hotfix, and create the branch from the last tag:\ngitGraph\n    commit id: \"abcd\"\n    commit id: \"efgh\" tag: \"0.14.0\"\n    branch feature1\n    branch feature2\n    branch hotfix-143\n    checkout main\n    checkout feature1\n    commit id: \"ijkl\"\n    commit id: \"mnop\"\n    checkout main\n    merge feature1 id: \"gpne\"\n    checkout feature2\n    commit id: \"qrst\"\n    commit id: \"uvwx\"\n    checkout hotfix-143\n    commit id: \"yzAB\"\nOnce you have made your hotfix (with a test of course, because we always make sure to verify our fixes), then you should push it up and open a pull request:",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Releasing Hotfixes</span>"
    ]
  },
  {
    "objectID": "hotfixes.html#step-1-create-a-fix-and-pull-request",
    "href": "hotfixes.html#step-1-create-a-fix-and-pull-request",
    "title": "14  Releasing Hotfixes",
    "section": "",
    "text": "git switch --detach 0.14.0 # checkout the tag\ngit switch -c hotfix-143 # create a new branch\n# do some work\ngit commit -m 'hotfix for #143'\n\n\ngit push -u origin hotfix-143",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Releasing Hotfixes</span>"
    ]
  },
  {
    "objectID": "hotfixes.html#step-2-run-integration-test-bump-the-version-and-add-a-tag",
    "href": "hotfixes.html#step-2-run-integration-test-bump-the-version-and-add-a-tag",
    "title": "14  Releasing Hotfixes",
    "section": "14.2 Step 2: Run Integration Test, Bump the Version and Add a Tag",
    "text": "14.2 Step 2: Run Integration Test, Bump the Version and Add a Tag\nYou should check that the hotfix does not break any existing tests AND you should check the Workbench Integration Test with the pull request you just created and the repository that is reporting the error.\nOnce you have confirmed that everything works as expected, the next step is to bump the version and add a tag. This tag will allow you to release the patch version.\nnano DESCRIPTION # bump the patch version\nnano NEWS.md     # add a new section describing the bug fix\ngit add DESCRIPTION NEWS.md\ngit commit -m 'bump version to 0.14.1'\ngit tag -s 0.14.1 -m 'hotfix for 143'\ngit push\ngit push --tags\n\n\n\n\n\ngitGraph\n    commit id: \"abcd\"\n    commit id: \"efgh\" tag: \"0.14.0\"\n    branch feature1\n    branch feature2\n    branch hotfix-143\n    checkout main\n    checkout feature1\n    commit id: \"ijkl\"\n    commit id: \"mnop\"\n    checkout main\n    merge feature1 id: \"gpne\"\n    checkout feature2\n    commit id: \"qrst\"\n    commit id: \"uvwx\"\n    checkout hotfix-143\n    commit id: \"yzAB\"\n    commit id: \"CDEF\" tag: \"0.14.1\"\n\n\n\n\n\n\nAt this point, the checks will not run on the pull request because there will be a conflict in the DESCRIPTION and NEWS.md files. This is okay. All you did was update the version numbers and add NEWS. The next step is to release the patch.",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Releasing Hotfixes</span>"
    ]
  },
  {
    "objectID": "hotfixes.html#step-3-release-the-patch",
    "href": "hotfixes.html#step-3-release-the-patch",
    "title": "14  Releasing Hotfixes",
    "section": "14.3 Step 3: Release the Patch",
    "text": "14.3 Step 3: Release the Patch\nYou will release the patch using the same method as described in the releases chapter. You can either release on GitHub directly or via the GitHub CLI. Importantly, when you create the release, you should create the release from the new tag:\ngh release create 0.14.1",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Releasing Hotfixes</span>"
    ]
  },
  {
    "objectID": "hotfixes.html#step-4-resolve-conflicts-and-merge",
    "href": "hotfixes.html#step-4-resolve-conflicts-and-merge",
    "title": "14  Releasing Hotfixes",
    "section": "14.4 Step 4: Resolve Conflicts and Merge",
    "text": "14.4 Step 4: Resolve Conflicts and Merge\nNow that you’ve created the release, you should resolve the conflicts in the DESCRIPTION and NEWS files and then merge it back into main (note: this will create two merge commits, but I’m only showing one in the diagram to make it cleaner):\n\n\n\n\n\ngitGraph\n    commit id: \"abcd\"\n    commit id: \"efgh\" tag: \"0.14.0\"\n    branch feature1\n    branch feature2\n    branch hotfix-143\n    checkout main\n    checkout feature1\n    commit id: \"ijkl\"\n    commit id: \"mnop\"\n    checkout main\n    merge feature1 id: \"gpne\"\n    checkout feature2\n    commit id: \"qrst\"\n    commit id: \"uvwx\"\n    checkout hotfix-143\n    commit id: \"yzAB\"\n    commit id: \"CDEF\" tag: \"0.14.1\"\n    checkout main\n    merge hotfix-143 id: \"DeFn\"\n\n\n\n\n\n\nNow you will have the patch in place for the released version and the devel version of The Workbench, which means that you can continue to develop as normal and merge the next feature when you are ready:\n\n\n\n\n\ngitGraph\n    commit id: \"abcd\"\n    commit id: \"efgh\" tag: \"0.14.0\"\n    branch feature1\n    branch feature2\n    branch hotfix-143\n    checkout main\n    checkout feature1\n    commit id: \"ijkl\"\n    commit id: \"mnop\"\n    checkout main\n    merge feature1 id: \"gpne\"\n    checkout feature2\n    commit id: \"qrst\"\n    commit id: \"uvwx\"\n    checkout hotfix-143\n    commit id: \"yzAB\"\n    commit id: \"CDEF\" tag: \"0.14.1\"\n    checkout main\n    merge hotfix-143 id: \"DeFn\"\n    checkout feature2\n    commit id: \"GHIJ\"\n    commit id: \"KLMN\"\n    checkout main\n    merge feature2 id: \"FnNL\"",
    "crumbs": [
      "Package Distribution",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Releasing Hotfixes</span>"
    ]
  },
  {
    "objectID": "remote/intro.html",
    "href": "remote/intro.html",
    "title": "15  Introduction",
    "section": "",
    "text": "15.1 Philosophy\nThe Workbench was built to be platform-independent, so that you could continue to deploy its features without relying on specific features of GitHub. We use the exact same two step process of building the markdown and then passing that to the HTML renderer, with a twist. The core deployment function in the workbench is sandpaper:::ci_deploy(), which will deploy the rendered markdown and the HTML to separate orphan branches in the Git repository that are mapped as git worktrees to the site/built and site/docs folders during the build process. In each step, when the build is successful, the results are pushed to the respective worktree before moving to the next step. When the process is done (regardless of outcome), the worktrees are torn down gracefully.\nflowchart TB\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    GH[(\"@main\")]:::remote\n    MDOUT[(\"@md-outputs\")]:::notouch\n    PAGES[(\"@gh-pages\")]:::notouch\n    DEPLOY([\"ci_deploy()\"]):::external\n    CIBUILDMD([\"ci_build_markdown()\"]):::external\n    CIBUILDSITE([\"ci_build_site()\"]):::external\n\n    subgraph virtual machine\n    REPO[\"[repo]\"]:::local\n    BUILT[\"[repo]/site/built\"]:::local\n    SITE[\"[repo]/site/docs\"]:::local\n    VLESS(\"validate_lesson()\"):::normal\n    BUILDMD([\"build_markdown()\"]):::normal\n    BUILDSITE([\"build_site()\"]):::normal\n    end\n\n    GH ---&gt; REPO\n    GH ~~~ DEPLOY\n    REPO -.- VLESS\n\n    DEPLOY ---&gt; VLESS\n    DEPLOY ---&gt; CIBUILDMD\n    DEPLOY ---&gt; CIBUILDSITE\n    VLESS -.- BUILDMD\n    CIBUILDMD ---&gt; MDOUT\n    MDOUT &lt;-.-&gt; BUILT\n    CIBUILDMD ---&gt; BUILDMD\n    CIBUILDSITE ---&gt; PAGES\n    PAGES &lt;-.-&gt; SITE\n    CIBUILDSITE ---&gt; BUILDSITE\n    BUILT -.- BUILDSITE\n    VLESS -.- BUILDSITE\n    BUILDMD --&gt; BUILT\n    BUILDSITE --&gt; SITE\nThis allows us to retain the commit history from building not just the HTML, but also the markdown outputs without interfering with the commit history for the lesson source. It also gives us the ability to use these branches as a cache so that the lesson doesn’t have to rebuild from scratch every time, but the biggest advantage is in the things that go beyond just deploying lessons.",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "remote/intro.html#sec-philosophy",
    "href": "remote/intro.html#sec-philosophy",
    "title": "15  Introduction",
    "section": "",
    "text": "15.1.1 Proof\nOne way to prove this works on any system that uses Git, you can create a lesson, push it to GitHub, and then render the contents before GitHub is done setting up its runners.\nThe file test-deploy.R will do just that1:\n\n# Load The packages --------------------------------------\nlibrary(sandpaper)\nlibrary(usethis)\nlibrary(withr)\nlibrary(ids)\nlibrary(fs)\n# Generate the Lesson ------------------------------------\ntmp &lt;- tempfile()\nid &lt;- paste0(\"TEST-\", adjective_animal(style = \"kebab\"))\ndir_create(tmp)\nlsn &lt;- path(tmp, id)\ncreate_lesson(lsn, open = FALSE)\n# Push the Lesson To Github ------------------------------\nwith_dir(lsn, {\n  use_github()\n})\n# Render and Deploy the Lesson ---------------------------\nwith_dir(lsn, {\n  sandpaper:::ci_deploy()\n})\n# Set GitHub Pages ---------------------------------------\nwith_dir(lsn, {\n  use_github_pages()\n})\n\nYou must run it in a non-interactive fashion:\nRscript remote/test-deploy.R\nNow you can visit the GitHub repository and if you wait ~30 seconds, GitHub will have created a website for you and will still be setting up the lesson engine. This shows that it is possible to deploy as long as you have the following:\n\nA system with {sandpaper} and Git set up properly\npush access to a remote Git repository\n\nIn fact, if you look at the example for ci_deploy(), you will see that it creates a lesson and remote repository and walks you through the process that happens.\nThe challenge when deploying a Workbench lesson then lies in the step of provisioning the virtual machine or docker container to build a lesson when it updates.",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "remote/intro.html#beyond-deployment",
    "href": "remote/intro.html#beyond-deployment",
    "title": "15  Introduction",
    "section": "15.2 Beyond Deployment",
    "text": "15.2 Beyond Deployment\nHaving a single workflow for deployment is fine, but in the context of a lesson that will generate its content, other tools are needed to avoid the element of surprise from taking over when a change is made to the lesson. On the converse side, tools are needed to bring in updates that can affect the security and accuracy of the lesson.\n\n15.2.1 Pull Request Management\nThe norm for working on GitHub is a trunk-based workflow—small branches containing different features or bug fixes are created and then merged into the default branch after review. If new content is added or packages update, it is important to have mechanisms to verify that the contents of a lesson and to intervene if something is incorrect before the changes happen.\n\n\n15.2.2 Updating Compontents\nThe update workflows are there because we understand that a data science lesson does not live in isolation and it cannot be built in isolation—contents and tools need to be updated as the software ecosystem changes. Thus, just like we provide the {sandpaper} functions sandpaper::update_cache() and sandpaper::update_github_workflows(), these are also available as GitHub workflows that will create a pull request (if it has permissions).",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "remote/intro.html#in-practice",
    "href": "remote/intro.html#in-practice",
    "title": "15  Introduction",
    "section": "15.3 In Practice",
    "text": "15.3 In Practice\nWe use GitHub Workflows to build and deploy our lessons2 and the rest of the chapters in this section will discuss how we set these up, but within the context of GitHub. Remember that our philosophy is that the workbench should be deployable anywhere. These workflows are responsible for provisioning GitHub’s Ubuntu 22.04 Runner Image with the packages and software needed to build a lesson with The Workbench.\n\n15.3.1 Workflows\nThere are broadly four categories of workflows, where an asterisk (*) denotes workflows that can be manually triggered by maintainers and a carrot (^) denotes workflows that require a personal access token to create a pull request\n\nDeployment* (sandpaper-main.yaml)\nPull Request Responders (pr-preflight.yaml, pr-receive.yaml)\nUpdates*^ (update-cache.yaml, update-workflows.yaml)\nPull Request Preview Managers (pr-comment.yaml, pr-close-signal.yaml, pr-post-remove-branch.yaml)\n\nThese workflows are individually documented in the sandpaper repository\nThese workflows are interrelated and have different triggers. Below are a set of diagrams that disambiguates these relationships. First up are the workflows that are run on a schedule and on demand. Note that the update workflows will only push to a branch if any updates exist, otherwise, they will exit silently.\n\n\n\n\n\nflowchart LR\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    WEEK[\\\"CRON weekly\"\\]:::remote\n    MONTH[\\\"CRON monthly\"\\]:::remote\n\n    subgraph MAIN WORKFLOW\n    push[\\\"push to main\"\\]:::remote\n    md-outputs[(\"md-outputs\")]:::local\n    gh-pages[(\"gh-pages\")]:::local\n\n    sandpaper-main.yaml:::normal\n    end\n\n    subgraph \"UPDATES (requires SANDPAPER_WORKFLOW token)\"\n    update-cache.yaml:::normal\n    update-workflows.yaml:::normal\n\n    update-cache[(\"update/packages\")]:::notouch\n    update-workflows[(\"update/workflows\")]:::notouch\n\n    PR[/\"pull request\"/]:::remote\n    end\n\n    push --&gt; sandpaper-main.yaml\n    WEEK --&gt; sandpaper-main.yaml\n    sandpaper-main.yaml -.-&gt;|\"pushes to\"| md-outputs\n    sandpaper-main.yaml -.-&gt;|\"pushes to\"| gh-pages\n    WEEK --&gt; update-cache.yaml\n    MONTH --&gt; update-workflows.yaml\n    update-cache.yaml -.-&gt;|\"pushes to\"| update-cache\n    update-workflows.yaml -.-&gt;|\"pushes to\"| update-workflows\n    update-cache.yaml -.-&gt;|\"creates\"| PR\n    update-workflows.yaml -.-&gt;|\"creates\"| PR\n\n\n\n\n\n\n\nNotice how none of the workflows push to main. The update workflows will push to the update/* branches and then create a pull request. It’s common to find workflows that will perform updates and then immediately push to the default branch (which is the case for the lesson-transition workflow), but it’s important to remember that a workflow that does automatic updates prevents the maintainers from critically inspecting the changes to the components. This is especially true of the update-cache.yaml workflow, which will update the {renv} lockfile. By passing it through the pull request process first, we can give the maintainers a way to audit the changes coming through.\n\n\n\n\n\nflowchart LR\n    subgraph PULL REQUEST\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    md-outputs[(\"md-outputs\")]:::local\n    PR[\\\"pull request\"\\]:::remote\n    pr-preflight.yaml:::normal\n    pr-recieve.yaml([\"pr-recieve.yaml\"]):::normal\n    pr-comment.yaml:::normal\n    pr-close-signal.yaml:::normal\n    pr-post-remove-branch.yaml:::normal\n    md-outputs-PR[(\"md-outputs-PR#\")]:::notouch\n    end\n\n    PR --&gt; pr-preflight.yaml\n    pr-preflight.yaml -.-&gt;|\"comments on\"| PR\n    pr-preflight.yaml ~~~ pr-recieve.yaml\n    PR --&gt;|\"on maintainer approval\"| pr-recieve.yaml\n    pr-recieve.yaml -.-|\"uses\"| md-outputs\n    pr-recieve.yaml -.-&gt;|\"triggers\"| pr-comment.yaml\n    pr-comment.yaml -.-&gt;|\"creates\"| md-outputs-PR\n    pr-comment.yaml -.-&gt;|\"comments on\"| PR\n    PR -.-&gt;|\"on close\"| pr-close-signal.yaml\n    pr-close-signal.yaml -.-&gt;|\"triggers\"| pr-post-remove-branch.yaml\n    pr-post-remove-branch.yaml -.-&gt;|\"deletes\"| md-outputs-PR\n\n\n\n\n\n\n\n\n\n15.3.2 Actions\nThese workflows use a series of Custom GitHub Actions (aside from the official GitHub actions of checkout and cache) which can be found in the following repositories:\n\nhttps://github.com/carpentries/actions a combination of both composite and JavaScript Actions that perform the duties for provisioning the workbench, provisioning packages for R-based lessons, validating pull requests, downloading data from previous runs, commenting on pull requests, and updating components.\nhttps://github.com/r-lib/actions similar to carpentries/actions, but these are used in our workflows to provision R (that is, set up the correct environment variables) and to provision pandoc. Many of these actions are designed for packages and we use them heavily in the workbench development.\nhttps://github.com/carpentries/create-pull-request a fork of a popular action that will create a pull request from a Github Workflow. This is a fork so that we can make sure that we will keep it secure.\n\nEach repository has the actions documented to a degree, but we will discuss the implications and design of the actions in a following chapter.",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "remote/intro.html#footnotes",
    "href": "remote/intro.html#footnotes",
    "title": "15  Introduction",
    "section": "",
    "text": "Please note: this will only work if you have a GitHub PAT set up so that {usethis} can interact with the GitHub API.↩︎\nGitHub can be a bit confusing with it’s terminology and fluid concepts. Their resource for Understanding GitHub Actions may help, but here’s how I think about it In this publication, whenever I refer to a GitHub Workflow, this is a YAML file that lives inside of a repository that tells GitHub how to set up its machine to build the lesson. It’s like a recipe plan and shopping list for a dinner. On the other hand, when I refer to a GitHub Action, this is a self-contained piece of software that will perform a specific task within a workflow. This is akin to a specific kitchen utensil, ingredient or spice within a recipe.↩︎",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "remote/deploy.html",
    "href": "remote/deploy.html",
    "title": "16  Deployment",
    "section": "",
    "text": "16.1 Provisioning R\nWe use the r-lib/actions/setup-r@v2 action to set up R’s environmental variables. The way we use it, it does not actually install R because R comes installed by default on GitHub’s Ubuntu runners 2, saving about a minute’s worth of installation time.\nAnother alternative is to use containers from the Rocker project or the base image of the R-universe if you want to work with Docker containers.",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "remote/deploy.html#provisioning-pandoc",
    "href": "remote/deploy.html#provisioning-pandoc",
    "title": "16  Deployment",
    "section": "16.2 Provisioning pandoc",
    "text": "16.2 Provisioning pandoc\nAgain, we use r-lib for this task, with the r-lib/actions/setup-pandoc@v2 action. Note: we use the default installation of pandoc, which is 2.19.2 as of this writing, and is expected to match the version of pandoc used in RStudio.\nAlternative ways to provision pandoc can be found in pandoc’s installation guide.",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "remote/deploy.html#sec-cache",
    "href": "remote/deploy.html#sec-cache",
    "title": "16  Deployment",
    "section": "16.3 Caching",
    "text": "16.3 Caching\nIn order to ensure that the process moves as rapidly as possible, we need to be able to cache the packages used in our workflow. This is a bit of a complex topic, but GitHub has written up a guide for caching dependencies. Effectively, our strategy for the cache is to ensure we can restore the packages that we have previously downloaded and (in the case of provisioning the workbench) installing only the updates we need. To do this, we need to define the folder to restore from and two keys:\n\nFolder: path to the R package library used, which for a normal R installation is the ${R_LIBS_USER} environment variable. For a {renv} cache, it will be in the ${RENV_PATHS_ROOT} environment variable.\nkey a very specific key to restore a valid cache that can be used immediately\nrestore-keys a less-specific key to restore an invalid cache that might be able to be updated\n\n\n16.3.1 Restoring an outdated cache with restore-keys\nIn the case of R packages, you need to be concerned about two things to restore a cache, valid or not:\n\nThe OS version\nThe R version\n\nRemember that R packages in the libary are built for the specific operating system and thus, if the operating system changes, the cache cannot be recovered. Similarly, if the R version has changed, a package may work, but there is no guarantee that it will work and thus the cache cannot be recovered.\nThus, when we define the prefix for the restore-keys and the key, we define it like this:\nrestore-keys: ${{ os-version }}-${{ r-version }}-\nThe important part of having the restore-keys is that, when it is hit, the cache will be updated when the workflow completes, thus recording the updates that were found.\n\n\n\n\n\n\ncache-version key\n\n\n\nYou might see reference to a $CACHE_VERSION key in some of the workflows. This is an outdated feature that will be removed in the future. There are times when a cache becomes borked and it needs to be reset quickly. Sometime in 2022, GitHub added the ability for maintainers to delete a cache with a button click in the “actions” tab, so this became trivial.\nBefore then, we had to rely on a trick that had maintainers register a CACHE_VERSION secret, which was recommended to set it to the date. This was tacked on to the end of the restore key so that if a cache needed to be reset, a maintainer could update the CACHE_VERSION secret and the cache would be invalidated completely.\n\n\n\n\n16.3.2 Restoring a valid cache with key\nA valid cache can be restored and used immediately without needing to reinstall any resources. To determine the validity of a particular cache, it’s important to understand where your updates are coming from.\n\n16.3.2.1 key For The Workbench\nIn terms of The Workbench, you want to make sure you pull in any updates from the R-universe and CRAN, because they will have any bugfixes that we did not consider. In practise, the way we do this is by saving a file from the output of remotes::package_deps(), which will get the recursive dependencies for The Workbench, check their package versions against the versions of the installed package and report which packages have updates available.\nHowever, you might be noticing something that is amiss: if you are checking for outdated packages before any packages are installed, then how does this help with invalidating the cache if you don’t know the package versions you have installed before you restore from the cache? The trick is that whenever you query the packages in this way, you are always comparing against a (near) empty R library, so the result will be the same across runs if and only if none of the packages have updated in the upstream repositories.\n\n\n\n\n\n\nExample: cowsay\n\n\n\nFor example, this is the output of checking for dependencies of the {cowsay} package:\n&gt; deps &lt;- remotes::package_deps(\"cowsay\")\n&gt; deps\nNeeds update -----------------------------\n package  installed available is_cran remote\n rmsfact  NA        0.0.3     TRUE    CRAN\n fortunes NA        1.5-4     TRUE    CRAN\n crayon   NA        1.5.2     TRUE    CRAN\n cowsay   NA        0.8.2     TRUE    CRAN\nWhen the data frame is saved in the workspace and the file is hashed, it will produce the same hash across builds because the installed the only column that will vary will be the available column (unless “cowsay” changes dependencies).\n\n\n\n\n\n\n\n\nIdea for the future\n\n\n\nIt might be possible for us to centralize this caching so that we run remotes::package_deps(\"sandpaper\") outside of the workflow in https://files.carpentries.org so that we can save a few seconds of time installing remotes and querying dependencies if the restore is successful.\n\n\n\n\n16.3.2.2 key for the {renv} package cache\nThe goal of the {renv} package cache is only to reliably restore it for reproducibility (say that five times fast), thus, the only key that we need to validate the cache is the renv/profiles/lesson-requirements/renv.lock file itself. If the hash for that is identical across runs, then the hash is valid and we can restore the cache as usual.",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "remote/deploy.html#sec-provision-workbench",
    "href": "remote/deploy.html#sec-provision-workbench",
    "title": "16  Deployment",
    "section": "16.4 Provisioning The Workbench",
    "text": "16.4 Provisioning The Workbench\nThe Workbench Packages are provisioned with the carpentries/actions/setup-sandpaper@main composite action.\nThis workflow does the following:\n\nSets up environment variables and options to allow us to fetch from the R-universe and RSPM\nFetches the system dependencies json record from the R-universe and installs them 3.\nRestores the Cache and Installs any missing/outdated dependencies\nInstalls any custom versions of sandpaper/varnish",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "remote/deploy.html#sec-provision-renv",
    "href": "remote/deploy.html#sec-provision-renv",
    "title": "16  Deployment",
    "section": "16.5 Provisioning The Package Cache",
    "text": "16.5 Provisioning The Package Cache\nThe {renv} package cache is provisioned with the carpentries/actions/setup-lesson-deps@main composite action. This action relies on the {vise} package to determine and install the system dependencies for the packages in the cache (e.g. it’s the reason why the spatial packages can be installed for the spatial lessons).\nThis has the following steps and is run only if the renv/ folder exists.\n\nSets up environment variables and options to allow us to fetch from the R-universe and RSPM\nDetermine and install system dependencies from the lockfile4\nprovision the packages with sandpaper::manage_deps()",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "remote/deploy.html#a-bit-of-history",
    "href": "remote/deploy.html#a-bit-of-history",
    "title": "16  Deployment",
    "section": "16.6 A bit of History",
    "text": "16.6 A bit of History\nIn theory, these can all be taken care of with a Docker container and, indeed, we have written a Dockerfile to do just this, piggybacking off of the R-universe base image. You might be wondering: why don’t we use a Docker image to build the lessons? Why do we use the runners for GitHub Actions? When we initially built The Workbench, building R packages on Ubuntu always required compilation, so we used macOS runners so that we could get compiled packages most of the time. The key point here is most of the time.\nThe release cycle of R packages on CRAN will release the source of the package first and build the binaries for macOS and Windows in the few days following. Importantly, these binaries would have the C libraries bundled with the packages that required them, so the installation would just work. During these few days, users will be prompted with a message asking them if they would like to install the binary version or compile the latest source. However, on GitHub runners, the machine always defaulted to the latest version, so sometimes, just after a package updated, we would get issues where a package (e.g. {stringi}) would fail to compile because the proper C library was not installed. This was especially problematic for a situation where we needed to provision an arbitrary set of packages for R-based lessons.\nIn November 2021, we officially switched our runners over to use Ubuntu with carpentries/actions#31 and carpentries/sandpaper#211. These allowed us to use the binary packages from the Posit Package Manger (previously RStudio Package Manager) to provision our builds and parse the necessary system dependencies.\nThis code initially lived inside of the github workflow, but the code got complex enough that it was worthwhile to port it to a specific package, which eventually became {vise}. This package was intended as a way to split off the {renv} system from {sandpaper} into its own package (akin to something like {capsule}, but I never had the bandwitdh to properly separate the {renv} components from the {sandpaper} components (though that may be easier now that the {flow} package exists for analysis of code pathways).",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "remote/deploy.html#footnotes",
    "href": "remote/deploy.html#footnotes",
    "title": "16  Deployment",
    "section": "",
    "text": "Perhaps not RedHat or CentOS, which are notoriously strict about updating their C libraries.↩︎\nsee https://github.com/carpentries/sandpaper/pull/279 for information about how we found that out.↩︎\nSee the ssl error of July 2023 for one consequence of this↩︎\nAt the moment, this is sort of complex because {remotes} package has not been updated on CRAN since 2021 and does not know about ubuntu 22.04, which is the version of runners that GitHub uses. We do this because installing from CRAN is faster.↩︎",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "remote/pull-request.html",
    "href": "remote/pull-request.html",
    "title": "17  Pull Request Responders",
    "section": "",
    "text": "17.1 Motivation\nMuch of the motivation for the pull request audits can be found in the Auditing Pull Requests episode in The Workbench documentation.\nflowchart TB\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    pr[\"Pull Request\"]:::remote\n    receive[\"Receive Pull Request\"]:::remote\n    validate[\"Check Valid PR\"]:::local\n    branch[\"Create md-outputs-{PR}\"]:::local\n    comment[\"Comment on Pull Request\"]:::local\n    \n    review[\"Maintainer Review\"]:::remote\n    \n    accept[\"Accepted\"]:::normal\n    reject[\"Rejected\"]:::notouch\n    deploy[\"Deploy\"]:::local\n    destroy[\"Destroy md-outputs-{PR}\"]:::local\n\n    pr --&gt; receive;\n    receive --&gt; validate;\n    validate -.-&gt;|\"valid\"| branch;\n    validate -.-&gt;|\"invalid\"| comment;\n    branch --&gt; comment\n    comment --&gt; review\n    review --&gt; reject \n    review --&gt; accept \n    accept --&gt; deploy \n    deploy ~~~ destroy\n    accept --&gt; destroy \n    reject --&gt; destroy",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Pull Request Responders</span>"
    ]
  },
  {
    "objectID": "remote/pull-request.html#preflight-checks",
    "href": "remote/pull-request.html#preflight-checks",
    "title": "17  Pull Request Responders",
    "section": "17.2 Preflight Checks",
    "text": "17.2 Preflight Checks",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Pull Request Responders</span>"
    ]
  },
  {
    "objectID": "remote/pull-request.html#pull-request-checks",
    "href": "remote/pull-request.html#pull-request-checks",
    "title": "17  Pull Request Responders",
    "section": "17.3 Pull Request Checks",
    "text": "17.3 Pull Request Checks",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Pull Request Responders</span>"
    ]
  },
  {
    "objectID": "remote/pull-request.html#clean-up",
    "href": "remote/pull-request.html#clean-up",
    "title": "17  Pull Request Responders",
    "section": "17.4 Clean up",
    "text": "17.4 Clean up",
    "crumbs": [
      "Remote Deployment and Management",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Pull Request Responders</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "18  Summary",
    "section": "",
    "text": "This book contains behind-the-scenes information, procedural information, and case studies of issues dealt with. It contains links to documentation that lives with the packages used by The Workbench, but it is not complete. There are several areas that are indeed missing and for that I apologise. I hope that this provides a start for someone who is wondering why The Workbench was designed the way it was and for insights about some of the processes that go on in building a lesson.\nThank you for reading.\nZhian N. Kamvar, 20 December, 2023",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Marwick, Ben. 2016. “Computational Reproducibility in\nArchaeological Research: Basic Principles and a Case Study of Their\nImplementation.” Journal of Archaeological Method and\nTheory 24 (2): 424–50. https://doi.org/10.1007/s10816-015-9272-9.\n\n\nVaniea, Kami, and Yasmeen Rashidi. 2016. “Tales of Software\nUpdates.” Proceedings of the 2016 CHI Conference on Human\nFactors in Computing Systems, May. https://doi.org/10.1145/2858036.2858303.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. “R\nPackages (2e).” https://r-pkgs.org/.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex\nNederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in\nScientific Computing.” Edited by Francis Ouellette. PLOS\nComputational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "sop.html",
    "href": "sop.html",
    "title": "Appendix A — Standard Operating Procedures",
    "section": "",
    "text": "A.1 Get a Reproducible Example\nThe most powerful tool in the developer’s toolbox for demonstrating an an issue or a new feature is the reproducible example. It is no surprise that Jenny Bryan (of “Everything I Know Is From Jenny Bryan” fame) is the maintainer of the {reprex} package and has an incredibly useful webinar on how to create reproducible examples with {reprex}.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Standard Operating Procedures</span>"
    ]
  },
  {
    "objectID": "sop.html#get-a-reproducible-example",
    "href": "sop.html#get-a-reproducible-example",
    "title": "Appendix A — Standard Operating Procedures",
    "section": "",
    "text": "System-Dependent Issues\n\n\n\nThere are times when you cannot create a reproducible example that will produce the results you want to see. This is often the case when there is a system-dependent issue. In these cases, you should share the reprex with someone who has the system in question and have them run it.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Standard Operating Procedures</span>"
    ]
  },
  {
    "objectID": "sop.html#sec-git-etiquitte",
    "href": "sop.html#sec-git-etiquitte",
    "title": "Appendix A — Standard Operating Procedures",
    "section": "A.2 Git/GitHub Etiquitte",
    "text": "A.2 Git/GitHub Etiquitte\nIn The Workbench practice, commits are never pushed to the main branch of the packages. All issues and features are fixed/made by pull requests because they give a better accounting of what was changed and why. Using this, it is good to strive for these practises1:\n\nIf an issue for the feature/bug does not exist, create it with documentation about where the error exists (or where the feature should be). Use the “copy permalink” when you click on a line number in the source view of GitHubto create a preview of the code as it existed before the feature/fix.\ncreate a new branch from main that describes the purpose and includes the issue number. For example: avoid-gert-config-449 removed a bit of code that was using the {gert} package to check for a git user, which was unnecessary.\npush the branch to GitHub2 and create a new draft pull request\nAdd a test that will reproduce the bug (if possible) before any code\nCOMMIT OFTEN. There are MANY times when I have been working on a feature deeply and have forgotten to commit, only to lose track of what changes I made and why3. Remember that git is your lab notebook.\nIterate writing code and committing until the tests for that particular function passes.\nWrite a new item in NEWS and bump the version in DESCRIPTION.\nPush to GitHub and mark as ready for review; tag @carpentries/maintainers-workbench\n\n\nA.2.1 Pull Request Reviews\nWhile The Workbench was being rapidly developed by Zhian Kamvar, all pull requests were merged by Zhian, even if he created them. After the launch of The Workbench across all official lessons, The Workbench Maintainer Group will now review all pull requests. In order to ensure pull request reviews are equitably given, please read Alex Hill’s blog post entitled “The Art of Giving and Receiving Code Reviews”.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Standard Operating Procedures</span>"
    ]
  },
  {
    "objectID": "sop.html#addressing-issues",
    "href": "sop.html#addressing-issues",
    "title": "Appendix A — Standard Operating Procedures",
    "section": "A.3 Addressing Issues",
    "text": "A.3 Addressing Issues\nOne big challenge with issues is that you cannot know where the issue is coming from. If you do nothing else, please watch this Keynote talk from Jenny Bryan: Object of type ‘closure’ is not subsettable. This will give you the right mindset for broadly approaching issues in R package development, which can be summarised as:\n\nTurn it off and on again (Restart R in a clean session)\nMake a reprex\nDig into the error (via traceback() and/or a debugger)\nFuture-proof your fixes and make things within reach\n\nWith these tools, if you cannot reproduce the error, you can guide the user to give you the information you need to reproduce the error and then you can follow the git guidelines to iterate on the fix.\n\n\n\n\n\n\nThink of the big picture\n\n\n\nAs you are fixing an issue, consider the larger picture of the issue. Sometimes you will run into an issue that will technically fix it, but still leave the possibility for future issues to open up. The most important thing is to document what future work needs to be done to ensure a robust solution while it is still fresh in your mind. This can be in an issue or in the comments of the code itself.\nSometimes an issue is urgent enough that a quick fix is what is needed, and in those cases, it’s best to open up a followup issue that addresses what would be needed for a more robust fix. Sometimes the solution is splitting a large function into smaller functions, but other times it may be a refactor of the internal data structure.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Standard Operating Procedures</span>"
    ]
  },
  {
    "objectID": "sop.html#creating-new-features",
    "href": "sop.html#creating-new-features",
    "title": "Appendix A — Standard Operating Procedures",
    "section": "A.4 Creating New Features",
    "text": "A.4 Creating New Features\nIf you want to create a new feature, the most important thing to do is to clearly define the audience, scope, dependencies, inputs and the outputs of the feature that you want to create. It’s best to strive for new features that bolster existing features (e.g. sandpaper::serve() provides an continously updating version of sandpaper::build_lesson()) or those that are modular and optional (e.g. the fail_on_error config option for lessons using R Markdown force an error in code blocks to stop the lesson from rendering unless those code blocks have the error = TRUE option).\nOne of the most important things to consider when adding new features is the maintenance and deployment workflow. Maintainers and contributors should not need to worry about function arguments, GitHub Actions, or new Markdown syntax in order to implement a new feature that will be deployed automatically to all of the lessons. A new feature should not break their workflow or the deployment workflow.\n\n\n\n\n\n\nSimple Example\n\n\n\nAs an example, when we were discussion folder organistaion, The original config template had a “schedule” section instead of “episodes”. When I replaced “schedule” with “episodes”, I added functionality to allow for old-style lessons to move forward so that any existing test lessons did not break. To this day, you can still create a lesson using schedule instead of episodes as the keyword.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Standard Operating Procedures</span>"
    ]
  },
  {
    "objectID": "sop.html#footnotes",
    "href": "sop.html#footnotes",
    "title": "Appendix A — Standard Operating Procedures",
    "section": "",
    "text": "I say strive for these practises because I do not always follow them myself, but they are there for very good reasons. For example: testing before you write code helps you design your code to do the thing you want it to do AND it helps avoid confirmation bias in the tests.↩︎\nIn RStudio, when you use the interface to create a new branch, it will generally also push the branch to GitHub for you. If it does not do this, you will need to push the branch with git push -u origin   &lt;BRANCH_NAME&gt;. I do this often enough that I’ve created a git alias (which can go in your ~/.gitconfig file):\npushb = !\"git push --set-upstream $(git remote | head -n 1) $(git symbolic-ref HEAD --short)\"↩︎\nYou do not have to push after each commit. Sometimes it is strategic to collect a little stack of local commits in case you were experimenting and need to revert back to one that gave you a working state.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Standard Operating Procedures</span>"
    ]
  },
  {
    "objectID": "flight-rules.html",
    "href": "flight-rules.html",
    "title": "Appendix B — Examples and Flight Rules",
    "section": "",
    "text": "B.1 Within The Workbench R Packages\nBugs or features in this category are entirely within our control and are theoretically the easiest/most quick implementations. Items in this category can be split into either single package items which can be fixed with a single pull request or cross-package items, which require coordination of pull requests to achieve.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Examples and Flight Rules</span>"
    ]
  },
  {
    "objectID": "flight-rules.html#sec-within",
    "href": "flight-rules.html#sec-within",
    "title": "Appendix B — Examples and Flight Rules",
    "section": "",
    "text": "B.1.1 Single Package\nIn this section, we outline issues that are addressed within a single package. Note that this does not indicate that these issues are straightforward to address.\n\nB.1.1.1 Single-Function Issues\nIf you are here, you have determined that the bug or feature that you are working on will affect a single function or data pathway. These issues are often the most straightforward to address. Below, I’ve documented narratives for these issues.\n\nMarkdown file for 404 page created with read-only permissions\n\ns479 Situation\n\nIssue: carpentries/sandpaper#479\nResolver: zkamvar\nType: Local\nOS: Rocky Linux 8.8\nPackage: sandpaper\n\nThe user is attempting to build a lesson, but they are unable to because an error appears during the “Create 404 page” step:\n── Creating 404 page ───────────────────────────────────────────────────────────────────────────────────────────────────\nError in file(file, ifelse(append, \"a\", \"w\")) : \n  cannot open the connection\nIn addition: Warning messages:\n[snip]\n7: In file(file, ifelse(append, \"a\", \"w\")) :\n  cannot open file '/tmp/RtmpmaiZ5B/file73cdc48b90540.md': Permission denied\n\n\ns479 Diagnosis\nThis permissions issue was a problem with copying a read-only file without adjusting the subsequent permissions. It stemmed from build_404() calling render_html() with the presence of a links.md document at the top of the lesson. Packages in the user’s R library are installed by the systems administrator and the user does not have permissions to write or append any file in that folder. When build_404() runs, it passes a template markdown file to render_html(). If there is an option called sandpaper.links set to a filepath, then this function will copy the input file to a temporary file and append the links to that temporary file before rendering it to HTML. Because the input file was read-only, copying the file to the temporary directory retained its permissions and we were unable to append the links.\n\n\ns479 Solution\nPull Request: carpentries/sandpaper#482\nThe solution was to add a single line adding user write permissions before the links file was appended: fs::file_chmod(tmpin, \"u+w\").\n\n\ns479 Alternative Solution\nThis additionally could have been avoided by temporarily unsetting the sandpaper.links option for the build_404() function before it calls render_html(). This would prevent it entering the loop where it appends the links, making the process slightly faster. The only downside is that we would need to do that for all of the other templated pages (though I believe that might be the only one).\n\n\ns479 Narrative\n\nIssue: carpentries/sandpaper#479\nFirst impression: this is running on a version of Linux that Zhian does not know anything about. My thoughts are that it will be a difficult fix.\nI suspect the bug might be due to the {fs} package and ask for the user if they could test out the following code snippet:\ntmp &lt;- fs::file_temp(ext = \".md\")\ncat(\"test\\n\", file = tmp, append = TRUE)\nreadLines(tmp)\nI looked again did not recognise the code snippet where the issue seemed to be (file(file, ifelse(append, 'a', 'w'))) - so this seems to be a problem with a non-Workbench package.\ninitially went looking for this code in the fs package, but GitHub search showed that this code doesn’t appear there either.\nnext guess is the cat function, which is used to add link references to the end of files. I searched the R code base and found the snippet in the cat()function\nthat seemed to be where that code snippet was coming from, but the problem really originated a few lines above the call to cat: when the template for the 404 page (which is saved where the package was installed) is copied—file_copy() copies a file and all of its permissions—so the copy is read-only for non-admin users.\nI opened a PR to test for the error, then applied a fix to prevent it from being thrown again.\nAsked the reporter to install the patch on their system and report back on whether it worked.\nThey reported back that it did—and pointed out a typo!\nAfter merging PR, create a new release (see process in The Release Workflow)\n\n\n\n\n\nB.1.1.2 Multi-Function Issues\n\n\nB.1.1.3 Test Failures With No User Impact\n\n\n\nB.1.2 Aross Packages\nSome features require manipulation across packages. Most often, this will mean a feature that is needed between {sandpaper} (user interface) and {varnish} (templates). To a rarer extent would you need features bewteen {sandpaper} and {pegboard}. You would almost never need a feature between {pegboard} and {varnish}.\n\nB.1.2.1 Order of Operations\nFeatures across packages require a bit of extra care because you are dealing with two moving targets. Remember that it’s not impossible to change features spanning packages in a backwards-compatible manner. If you are worried that something will break, I will remind you that not only do we have unit tests, but we also have The Workbench integration tests that allows us to test development versions of The Workbench across a variety of lessons. On top of that, you have the ability to set the versions of The Workbench used to build any given lesson. With these three tools, you should be able to address any issue that arises from a multi-package feature/issue.\nThis section talks about the order of operations in testing and release, which can be boiled down to these general rules:\n\nAlways test {sandpaper} against the development version of the other package\nRelease the dependent package first. Unless you have made breaking changes, {varnish} and {pegboard} should be released before {sandpaper}.\n\nWith that, here are scattered details of the order of operations for developing features for two packages.\nMake sure to use the Remotes field in the {sandpaper} DESCRIPTION file to ensure that you are testing against the correct version of whatever other package you are using. That is, if you are testing feature-a in {sandpaper} and there is a companion feature-a branch in {varnish}, you want to specify Remotes: carpentries/varnish@feature-a in the DESCRIPTION file for {sandpaper}. This will not only allow the unit tests to work against the correct version of the dependent package, but it will also allow the integration tests and user tests to work.\nIn general, when you are creating a feature across two packages you will want to ensure the following:\n\nthere is an issue open in the workbench repository\nthe branch names for each feature are identical (or nearly so)\nthe pull request for {sandpaper} sets the Remotes\nthe dependent package is released before sandpaper.\n\n\n\nB.1.2.2 Examples\n\nAdding DOI badge for peer reviewed lessons\nReplace {pkg} with the first letter of the package the issue occurs in and {num} with the issue number\n\ns535v97 Situation\n\nIssue: carpentries/sandpaper#535, carpentries/varnish#97\nResolver: tobyhodges, zkamvar\nType: Local\nOS: NA\nPackage: {sandpaper}, {varnish}\n\n\n\ns535v97 Diagnosis\nIn carpentries/workbench#67 and carpentries/varnish#40, Toby Hodges had pointed out that while we had badges for lessons in pre-alpha, alpha, and beta phases of development, we did not yet have a badge for a published lesson with a DOI.\nIn the former infrastructure, we had indicated these statuses by adding a banner to the top of the lesson. This banner would take up extra room at the top of the lesson, so we changed it to a badge since we had room next to the logo. In this case, Toby was asking if we could add a doi key to the config.yaml that would allow the badge to be displayed on the website.\n\n\ns535v97 Solution\nToby had opened two pull requests on 2023-10-30:\n\ncarpentries/sandpaper#535 where he modified the inst/templates/pkgdown-yaml-template.txt and create_pkgdown_yaml() to include a doi element.\ncarpentries/varnish#97 where he added HTML to the header that would insert a stylized DOI badge\n\nOn 2023-11-10, Zhian Kamvar added a catch for full URL of a DOI vs the raw DOI itself in {sandpaper} and determined that the contribution to {varnish} needed a couple of extra tweaks before being merged.\n{varnish} was merged at 11:43AM and {sandpaper} was merged at 11:47AM\n\n\ns535v97 Alternative Solution\nOne of the tools that Toby had used was passing data through create_pkgdown_yaml(). It could have been done without that function and just passed through initialize_metadata() or something similar. For details on how data flows from {sandpaper} to {varnish}, read Data Flow from Source to Website in the {sandpaper} documentation.\nIt’s important to understand why Zhian used create_pkgdown_yaml() in the first place: to get {pkgdown} to recognise the site/ folder as a package so he could hijack {pkgdown}’s machinery to provision a website. When he first started, he did not realize that he could pass any number of data elements directly to the {varnish} templates (as is done through the global variable caches), so everything was written to site/_pkgdown.yaml. This was cumbersome because it involves the IO for the pkgdown template file, the config file, and the resulting pkgdown yaml. It was much easier to load the config file via initialize_metadata() and use that to carry the configuration items.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Examples and Flight Rules</span>"
    ]
  },
  {
    "objectID": "flight-rules.html#sec-upstream",
    "href": "flight-rules.html#sec-upstream",
    "title": "Appendix B — Examples and Flight Rules",
    "section": "B.2 Upstream R Packages",
    "text": "B.2 Upstream R Packages\n\nB.2.1 renv\n\nB.2.1.1 Background\nThe {renv} package is a key player for allowing {sandpaper} to provision and maintain the R packages required to build R-based lessons. The motivation and strategy for how it works can be found in the Building Lessons With A Package Cache article in the {sandpaper} docuementation.\nIt’s worth diving into carpentries/sandpaper#21 to see the discussion and thoughts around the origin of the design for using this feature. It was implemented during a three week period between 2021-08-24 and 2021-09-16, as detailed in the pull request carpentries/sandpaper#158.\n\n\nB.2.1.2 In practices\nWe have to consider {renv} in practice from the standpoint of both local computers and on GitHub, which can behave very differently and require different tools to address their tasks.\nThere are three tools and packages that use {renv}:\n\n{sandpaper} is designed to provide a way to manage dependencies in a lesson\n{vise} was originally intended as a project to split out {sandpaper} code that used {renv} to simplify the testing. At the moment, it provides utilities for automatically provisioning C libraries on Ubunutu Linux and running the equivalent of sandpaper::update_cache() in a GitHub Actions context.\ncarpentries/actions these contain R code within YAML files 😱 that will call {renv} and {vise}.\n\n\n\nB.2.1.3 Debugging Tips\n\nB.2.1.3.1 Setting up a reproducible environment\nBrowse the {renv} issues opened by @zkamvar. In nearly all of these issues, I provide a reproducible example. They generally follow the pattern of:\n\ncreate a temporary file\nmake it a directory and move there\nadd any files that are needed before the renv project is set up (if specific to problem)\nset up a {renv} project with renv::init()\ndemonstrate problem\n\ntmp &lt;- tempfile()\ndir.create(tmp)\nsetwd(tmp)\nwriteLines(\"library(R6)\", \"test.R\")\nrenv::init(profile = \"test\")\n# demonstrate problem here\n\n\nB.2.1.3.2 Choosing a minimal example\nPresenting a failing CI run with a Workbench lesson is reproducible, but it’s not minimal. Presenting the same with a smaller lesson is still not minimal. Often times, the issue involves detecting and installing a new package. In this case, you want to choose a package that has few to no dependencies and is not listed as a dependency for knitr. One example that I use often is the {cowsay} package. It has a total of three dependencies and is not depended on by anything. If I need a quick package with zero dependencies, I will reach for {R6} or {aweek}. Both of these packages are under 100 Kilobytes, are pure R code, do not have any dependencies, and do not require compillation.\n\n\nB.2.1.3.3 When you just can’t reproduce it locally\nThere have been times when {renv} seems to fail only on GitHub Actions. This was the case for {renv} version 0.17.0, as I reported in rstudio/renv#1161.\nThe important thing in these situations is to stay calm and try to narrow down as much as possible the exact conditions that will create the problem. Once you have those, you can open an issue on the {renv} issue tracker. If you are at this point, it’s important to not expect this to be resolved quickly, because it is likely out of your control. The best you can do is to try the debugging techniques that Kevin provides and report back on the issue thread.\nOnce the issue is resolved: Thank Kevin for his help. This is a very important point. Maintainers often only hear from their users if somehting is going wrong, so it’s important to let them know that they are appreciated.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Examples and Flight Rules</span>"
    ]
  },
  {
    "objectID": "flight-rules.html#sec-actions",
    "href": "flight-rules.html#sec-actions",
    "title": "Appendix B — Examples and Flight Rules",
    "section": "B.3 GitHub Actions",
    "text": "B.3 GitHub Actions\nThere is a category of issues that fail explicitly with GitHub actions only. As with the other issues, it is important not to panic when you encounter these, but instead, take stock of what happens when these happen. In general, here are the steps you should take to diagnosing problems:\n\nPrepare your inbox to filter messages from GitHub. A good way is to check that the subject line matches “Run failed 01 Build and Deploy” or “Run failed 03 Update Cache”.\nFind and read the error message from the log. You can find this by looking for the red X and expanding that section. It will normally scroll down to the error.\nRead the context of the error message. Most errors happen because of problems upstream, so it’s important to see what was happening when that error happened\nRestart the action. GitHub has a button near the top right of the action. If it’s ephemeral (e.g. networking error) then it will run fine on the next run.\nTest the build locally\nTest it on the carpentries/sandpaper-docs repository\nTest it on a brand new repository created via the templates (https://bit.ly/new-lesson-md or https://bit.ly/new-lesson-rmd)\n\nBelow I will attempt to outline common problem areas and their potential solutions. Note: this is not an exhaustive list, but you have to remember that in these situations, you are effectively debugging someone else’s computer, so patience is absolutely required because this is more difficult than plucking a single grain of rice with metal chopsticks.\n\nB.3.1 Networking Failures\nThese are often the easiest problems to solve. You will encounter an error that says somthing like “network timed out” and it fails when either checking out the repository or installing pandoc. In these situations, you should check https://status.github.com to see if this is a problem with GitHub, and then you should restart the build. Most of the time, the build will work on the second try.\n\n\nB.3.2 Upstream System Dependency Issues\n\nSSL error\n\nbioc-intro109 Situation\n\nIssue: https://github.com/carpentries-incubator/bioc-intro/issues/109\nResolver: NA\nType: Remote\nOS: Ubuntu 22.04\nPackage: NA\n\nOn 2023-07-20, @lgatto, who maintains the carpentries-incubator/bioc-intro repository reported an error in their builds during the “Setup Package Cache” of the sandpaper-main.yaml workflow:\nRegister Repositories\nUsing github PAT from envvar GITHUB_PAT\nError: Error: Failed to install 'vise' from GitHub:\n  SSL peer certificate or SSH remote key was not OK: [api.github.com] SSL: no alternative certificate subject name matches target host name 'api.github.com'\nExecution halted\nError: Process completed with exit code 1.\nThis error was causing the carpentries/vise package to not be installed and the {renv} package cache could not be provisioned for lessons. Because it involved the {renv} package cache, this was a problem that was limited to R-based lessons.\n\n\nbioc-intro109 Diagnosis\nThe cause of the problem was identified by Gabor Csardi as a bug specific to the development version of curl on Ubuntu: https://bugs.launchpad.net/ubuntu/+source/curl/+bug/2028170.\nThe key error message here was SSL: no alternative certificate subject name matches target host name 'api.github.com'. The curl program was checking the validity of the SSL certificate for github.com, but it was ignoring the rules for *.github.com, so when it found api.github.com, it saw that as an invalid site that should not be trusted.\nEven though the default version of curl on GitHub’s runners is not the dev version, when we provision the workbench in the “Setup Lesson Engine” step, we query the system dependencies from the R-universe:\n$ curl https://carpentries.r-universe.dev/stats/sysdeps 2&gt; /dev/null \\\n  | jq -r '.headers[0] | select(. != null)'\nlibcurl4-openssl-dev\nlibfontconfig-dev\nlibfreetype-dev\nlibfribidi-dev\nlibharfbuzz-dev\nlibicu-dev\nlibgit2-dev\nlibjpeg-turbo8-dev\nlibpng-dev\nlibtiff-dev\nlibxml2-dev\nlibxslt1-dev\nlibssl-dev\nThis installed the development version of curl into our system and introduced the bug during the “Setup Package Cache” step.\n\n\nbioc-intro109 Solution\nThe solution was to wait for a fix.\n\n\nbioc-intro109 Alternative Solution\nAn alternative solution that we tested was to switch the download method for R packages to “wget”, but this was a non-starter because this caused the installation times for R packages and the workbench to rise because they would need to be compiled (posit PPM did not serve the binary packages over wget).\n\n\nbioc-intro109 Narrative\n\nI get the notification of the issue and @lgatto suspects it’s because of {vise}.\nI install {vise} using remotes::install_github(\"carpentries/vise\") to check that it’s installable and I am successful.\nI rerun the action and get the same error. I run the action on the carpentries/sandpaper-docs repository, but it also fails similarly. This confirms that it’s a broader issue.\nI thought maybe also it was a problem in the {remotes} package since it hasn’t been updated on CRAN since 2021.\nI found https://github.com/r-lib/remotes/issues/762 which points to an issue in utils::download.file() as the source of the problems with curl.\nI notice that {vise} is still recorded as “zkamvar/vise” in the github actions files. I change it in carpentries/actions@2de7e3fef\nThis does not fix the issue and I post to mastodon: https://fosstodon.org/@zkamvar/110741428815173845\nI update the installation for vise to use “wget” as a fallback (see this comparison of commits\nI create a new branch in carpentries/actions to set options(download.file.method = \"wget\") in setup-sandpaper (view the diff).\nI create https://github.com/zkamvar/2023-07-19-test-actions, which fails initially and replace all @main declarations in .github/workflows/ with @2023-07-ssl-errors to test the fix that I created. The next run also fails and it takes &gt; 17 minutes to set up the workbench (which normally takes ~1 minute or less). I understand that the “wget” method will not work.\nGabor Csardi identifies the issue as coming from the dev version of curl\nI check the version of curl in the ubuntu runner image, which I found by expanding “Setup Job” and then “Runner Image” in the GitHub action run and finding the URL above, but it is not devel.\nI create https://github.com/zkamvar/test-github-actions-ssl to as a MWE of the error.\nI create a workflow that runs the command from the ubuntu bug report and find that it works.\nI update the workflow so that it demonstrates the working example and then demonstrates the failing example. It successfully fails\nI wait and check the bug has been fixed for ubuntu and I rerun the workflow. It succeeds and I re-run the bioc-intro and sandpaper-docs builds to find that they work.\n\n\n\n\n\nB.3.3 Workflow Mis-Configuration\nSometimes the workflow files themselves are mis-configured. In these cases, the fix will involve updating the files in sandpaper’s inst/workflows/ directory. If the pr-comment.yaml or update-workflows.yaml files are not affected, then a pull request will be created automatically to all lessons within a week after you submit the patch, but if these files are affected, then you will have to manually submit the patch.\n\nfiles with spaces in names cause pr-comment.yaml workflow to fail\n\ns399 Situation\n\nIssue: carpentries/sandpaper#399\nResolver: zkamvar\nType: Remote\nOS: Ubuntu 22.04\nPackage: sandpaper\n\n\n\ns399 Diagnosis\n\n\ns399 Solution\n\n\ns399 Alternative Solution\n\n\ns399 Narrative\n\n\n\n\nB.3.4 Permissions Changes\n\nB.3.4.1 carpentries-bot\n\n\n\nB.3.5 Actions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Examples and Flight Rules</span>"
    ]
  },
  {
    "objectID": "flight-rules.html#sec-structural",
    "href": "flight-rules.html#sec-structural",
    "title": "Appendix B — Examples and Flight Rules",
    "section": "B.4 Structural Features",
    "text": "B.4 Structural Features",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Examples and Flight Rules</span>"
    ]
  }
]